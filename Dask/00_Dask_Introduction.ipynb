{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Anaconda\"](img/anaconda-logo.png)\n",
    "\n",
    "*Copyright Continuum 2012-2016 All Rights Reserved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"img/dask_horizontal.svg\" align=\"right\" width=\"30%\">\n",
    "\n",
    "Dask is a library for parallel computing on larger-than-memory data.\n",
    "\n",
    "Dask emphasizes the following virtues:\n",
    "*  **Familiar**: Provides parallelized NumPy array and Pandas DataFrame objects\n",
    "*  **Native**: Enables distributed computing in Pure Python with access to the PyData stack.\n",
    "*  **Fast**: Operates with low overhead, low latency, and minimal serialization necessary for fast numerical algorithms\n",
    "*  **Flexible**: Supports complex and messy workloads\n",
    "*  **Scales up**: Runs resiliently on clusters with 100s of nodes\n",
    "*  **Scales down**: Trivial to set up and run on a laptop in a single process\n",
    "*  **Responsive**: Designed with interactive computing in mind it provides rapid feedback and diagnostics to aid humans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Dask: Introduction](#Dask:-Introduction)\n",
    "* [Background](#Background)\n",
    "\t* [Data Structures](#Data-Structures)\n",
    "\t* [The Problem](#The-Problem)\n",
    "\t* [The Solution](#The-Solution)\n",
    "* [Two Levels of Dask](#Two-Levels-of-Dask)\n",
    "\t* [High-Level](#High-Level)\n",
    "\t* [Low Level](#Low-Level)\n",
    "* [High-Level Interfaces, Mimicry of Familiar Containers](#High-Level-Interfaces,-Mimicry-of-Familiar-Containers)\n",
    "\t* [Dask DataFrame](#Dask-DataFrame)\n",
    "\t* [Dask Array](#Dask-Array)\n",
    "\t* [Dask Bag](#Dask-Bag)\n",
    "\t* [Dask Delayed](#Dask-Delayed)\n",
    "* [Low-Level Machinery: Task Graphs and Schedulers](#Low-Level-Machinery:-Task-Graphs-and-Schedulers)\n",
    "\t* [Graphs](#Graphs)\n",
    "\t* [Scheduling](#Scheduling)\n",
    "\t* [Profiling](#Profiling)\n",
    "* [Demonstration](#Demonstration)\n",
    "* [Further Reading](#Further-Reading)\n",
    "\t* [Reference Links](#Reference-Links)\n",
    "\t* [Dask Tutorial](#Dask-Tutorial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyData is largely a software community based on 2 data structures:\n",
    "* Numpy Array (N-dimensional array)\n",
    "* Pandas DataFrame (in-memory relational database table)\n",
    "\n",
    "> *\"The fact that we all depend on these 2 data structures makes it so that all of our libraries interoperate very effectively. \n",
    "We have plotting libraries, data loading libraries, data analysis libraries, \n",
    "All these libraries can inter-operate without having to communicate with each other. \n",
    "Developers on indepedent teams can work independently without worrying about what other dev groups are doing.\"* -- Matt Rocklin, core developer on the Dask project, at [PyData Seattle 2015 (YouTube)](https://www.youtube.com/watch?v=ieW3G7ZzRZ0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two data structures have two big flaws:\n",
    "* Mostly only handle data that fits into memory (RAM)\n",
    "* Mostly only use a single core\n",
    "\n",
    "Problems:\n",
    "* As hard-drives get big and support larger data files...\n",
    "* we want to reach outside of memory and **use the hard-drive**, not just the RAM\n",
    "* New machine have more and **more cores** we are not using\n",
    "* Imbalance between what our core data structures can do, and how good hardware is getting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is a library for parallel computing on larger-than-memory data\n",
    "* local dask can handle data of size (1 GB < data < 1 TB)\n",
    "    * using your laptop more effectively\n",
    "* distributed dask can handle much more\n",
    "    * running jobs on a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Levels of Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways to interact with Dask\n",
    "* high-level API (that mimics Numpy and Pandas)\n",
    "* low-level machinery (Delayed computation of task graphs with dynamic, low-latency task scheduler)\n",
    "\n",
    "Let's talk about both high and low level, interleaved.\n",
    "* Talk about the high level pandas like thing, and then\n",
    "* discussion about how that is enabled by the low level machinery.\n",
    "\n",
    "From High to Low:\n",
    "* Dask Array/DataFrame takes your high-level code, and turns it into a \"recipe\"\n",
    "* This \"recipe\" is a task graph of a bunch of different small functions to run in memeory\n",
    "* Dask schedulers then execute these task graphs in parallel, using many cores, many threads, many processes on many nodes in a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask at a High-Level interactions:\n",
    "* Numpy mimic, called \"Dask Array\" (numpy array + threading)\n",
    "* Pandas mimic, called \"Dask DataFrame\" (pandas dataframe + threading)\n",
    "* Python (parallel) List mimic, called \"Dask Bag\" (map, filter, + multi-processing)\n",
    "* All of these rely on the same low-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to write code up at this high-level:\n",
    "\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "df = dd.read_csv('2015-*.csv')\n",
    "df.groupby(df.user_id).value.mean().compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask schedulers execute the task graphs in parallel, using many cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"img/fail-case.gif\" width=60% align=\"right\">\n",
    "Dask at a Low-Level technology:\n",
    "* Dynamic and low-latency task scheduler\n",
    "* Aware of memory\n",
    "* Accessible\n",
    "* Arbitrary and simple graph structure\n",
    "* Dask schedulers provide custom parallelism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Level Interfaces, Mimicry of Familiar Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dask collections are the main interaction point for users. \n",
    "* They look like NumPy and pandas but generate dask graphs internally. \n",
    "* If you are a dask *user* then you should start here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dask DataFrame mimics the Pandas DataFrame API:\n",
    "\n",
    "```python\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "df = pd.read_csv('2015-01-01.csv')\n",
    "df.groupby(df.user_id).value.mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Dask Mimics Pandas\n",
    "import dask.dataframe as dd\n",
    "df = dd.read_csv('2015-*-*.csv')\n",
    "df.groupby(df.user_id).value.mean().compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dask Array mimics the Numpy ndarray API:\n",
    "\n",
    "```python\n",
    "# Numpy\n",
    "import numpy as np \n",
    "f = h5py.File('myfile.hdf5')\n",
    "x = np.array(f['/small-data'])\n",
    "x - x.mean(axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Dask mimics Numpy\n",
    "import dask.array as da\n",
    "f = h5py.File('myfile.hdf5')\n",
    "x = da.from_array(f['/big-data'], chunks=(1000, 1000))\n",
    "x - x.mean(axis=1).compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dask Bag is like a Python parallel list, and mimics iterators, Toolz, PySpark\n",
    "\n",
    "```python\n",
    "import dask.bag as db\n",
    "b = db.read_text('2015-*-*.json.gz').map(json.loads)\n",
    "b.pluck('name').frequencies().topk(10, lambda pair: pair[1]).compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dask Delayed structure mimics ***`for loops`*** and wraps custom code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "from dask import delayed\n",
    "L = []\n",
    "for fn in filenames:            # Use for loops to build up computation\n",
    "   data = delayed(load)(fn)          # Delay execution of function\n",
    "   L.append(delayed(process)(data))  # Build connections between variables\n",
    "\n",
    "result = delayed(summarize)(L)\n",
    "result.compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-Level Machinery: Task Graphs and Schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask represents parallel computations with [task graphs](graphs.md).  \n",
    "\n",
    "These directed acyclic graphs (DAG) may have arbitrary structure\n",
    "\n",
    "* freedom to build sophisticated algorithms\n",
    "* can handle messy situations not easily managed by the ``map/filter/groupby`` paradigm common in most data engineering frameworks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dask collections and schedulers](img/collections-schedulers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask Graphs\n",
    "\n",
    "* Dask graphs encode algorithms in a simple format involving Python dicts, tuples, and functions.\n",
    "* This graph format can be used in isolation from the dask collections.\n",
    "* Working directly with dask graphs is an excellent way to implement and test new algorithms in fields such as linear algebra, optimization, and machine learning.  \n",
    "\n",
    "\n",
    "If you are a *developer*, you should start here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schedulers execute task graphs.  \n",
    "\n",
    "Dask currently has two main schedulers:\n",
    "\n",
    "* one for single machine processing using threads or processes\n",
    "* and one for distributed memory clusters.\n",
    "\n",
    "Documentation from [dask.pydata.org](http://dask.pydata.org):\n",
    "* [Scheduler Overview](http://dask.pydata.org/en/latest/scheduler-overview.html)\n",
    "* [Single-Machine Scheduler](http://dask.pydata.org/en/latest/shared.html)\n",
    "* [Many-Machine Scheduler (aka \"dask.distributed\")](http://distributed.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask provides a few tools to help make debugging and profiling graph execution easier.\n",
    "\n",
    "Documentation from [dask.pydata.org](http://dask.pydata.org):\n",
    "* [Inspecting](http://dask.pydata.org/en/latest/inspect.html)\n",
    "* [Diagnostics](http://dask.pydata.org/en/latest/diagnostics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the numpy interface, as it will be mimiced by Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for demo by opening a system utility to monitor usage of CPU and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linux/Mac: from the terminal:\n",
    "# ! top -o mem -O cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do some math and watch how system resources are used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Recall the np.random module:\n",
    "#     * array of random samples from a gaussian distribution\n",
    "#     * median=`loc`, stddev=`scale`, number of samples = `size`\n",
    "\n",
    "array = np.random.normal(loc=10, scale=1.0, size=1000 )\n",
    "array[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e9d479efd429>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'array' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use numpy as our baseline, to set expectations for the time and resources needed to draw a larger number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.00000147880727"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "# N = int(2e7) # <1 sec\n",
    "N = int(2e8) # <10 sec\n",
    "#N = int(2e9) # <100 sec\n",
    "\n",
    "np.random.normal(10, 0.1, size=(N,) ).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do it with Dask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the interface and compare with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000002169108269"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "#N = int(2e7) # 0.1 sec\n",
    "#N = int(2e8) # 2 sec\n",
    "N = int(2e9) # 15+ sec\n",
    "\n",
    "chunk_size=int(1e6)\n",
    "da.random.normal(10, 0.1, size=(N,), chunks=(chunk_size,) ).mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is part of the Blaze project supported and offered by\n",
    "Continuum Analytics and others.\n",
    "\n",
    "* [Blaze](http://continuum.io/open-source/blaze)\n",
    "* [Continuum Analytics](http://continuum.io)\n",
    "* [3-clause BSD license](https://github.com/dask/dask/blob/master/LICENSE.txt)\n",
    "* [`#dask tag`](http://stackoverflow.com/questions/tagged/dask)\n",
    "* [GitHub issue tracker](https://github.com/dask/dask/issues)\n",
    "* [blaze-dev@continuum.io](https://groups.google.com/a/continuum.io/forum/#!forum/blaze-dev)\n",
    "* [gitter chat room](https://gitter.im/dask/dask)\n",
    "* [xarray](http://xray.readthedocs.org/en/stable/)\n",
    "* [scikit-image](http://scikit-image.org/docs/stable/)\n",
    "* [scikit-allel](https://scikits.appspot.com/scikit-allel)\n",
    "* [pandas](http://pandas.pydata.org/pandas-docs/version/0.17.0/)\n",
    "* [Distributed scheduler](http://distributed.readthedocs.org/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links:\n",
    "* [Matt Rocklin, core Dask developer, at PyData Seattle 2015 (YouTube)](https://www.youtube.com/watch?v=ieW3G7ZzRZ0)\n",
    "* [Tutorial Repository (GitHub)](https://github.com/dask/dask-tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-Up:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/dask/dask-tutorial\n",
    "cd dask-tutorial\n",
    "python ./prep.py  # create artificial data sets\n",
    "\n",
    "conda install dask pandas\n",
    "pip install castra graphviz  # optional, for graphs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Copyright Continuum 2012-2016 All Rights Reserved.*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
