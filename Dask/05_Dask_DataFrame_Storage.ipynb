{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Anaconda\"](img/anaconda-logo.png)\n",
    "<br>\n",
    "*Copyright Continuum 2012-2016 All Rights Reserved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/hdd.jpg\" width=\"20%\" align=\"right\">\n",
    "\n",
    "Efficient storage can dramatically improve performance, particularly when operating repeatedly from disk.\n",
    "\n",
    "* Decompressing text and parsing CSV files is expensive.\n",
    "* One of the most effective strategies with medium data is to use a binary storage format like HDF5.  \n",
    "* Often the performance gains from doing this are sufficient so that you can switch back to using Pandas again instead of using `dask.dataframe`.\n",
    "\n",
    "In this section we'll learn how to efficiently arrange and store your datasets in on-disk binary formats.  We'll use the following:\n",
    "\n",
    "1.  [Pandas `HDFStore`](http://pandas.pydata.org/pandas-docs/stable/io.html#io-hdf5) format on top of `HDF5`\n",
    "2.  Categoricals for storing text data numerically\n",
    "3.  [Castra](http://github.com/Blosc/castra), an experimental data store optimized for `dask.dataframe`\n",
    "\n",
    "**Main Take-aways**\n",
    "\n",
    "1.  Storage formats affect performance by an order of magnitude\n",
    "2.  Text data will keep even a fast format like HDF5 slow\n",
    "3.  A combination of binary formats, column storage, and partitioned data turns one second wait times into 80ms wait times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [DataFrame Storage](#DataFrame-Storage)\n",
    "\t* [Set-up](#Set-up)\n",
    "* [Read CSV](#Read-CSV)\n",
    "* [Write to HDF5](#Write-to-HDF5)\n",
    "* [Compare CSV to HDF5 speeds](#Compare-CSV-to-HDF5-speeds)\n",
    "\t* [1.  Store text efficiently with categoricals](#1.--Store-text-efficiently-with-categoricals)\n",
    "\t* [2.  Store columns separately with Castra](#2.--Store-columns-separately-with-Castra)\n",
    "\t* [Castra and partitioned indexes](#Castra-and-partitioned-indexes)\n",
    "* [Exercise](#Exercise)\n",
    "\t* [Solutions](#Solutions)\n",
    "* [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data if we don't have any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from src.dask_prep import accounts_csvs\n",
    "accounts_csvs(3, 1000000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we read our csv data as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "filename = os.path.join('tmp', 'accounts.*.csv')\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_csv(filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas contains a specialized HDF5 format, `HDFStore`.  The ``dd.DataFrame.to_hdf()`` method works exactly like the ``pd.DataFrame.to_hdf()`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = os.path.join('tmp', 'accounts.h5')\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time df.to_hdf(target, '/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = dd.read_hdf(target, '/data')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare CSV to HDF5 speeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a simple computation that requires reading a column of our dataset and compare performance between CSV files and our newly created HDF5 file.\n",
    "\n",
    "Which do you expect to be faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time df.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time df2.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly they are about the same.  \n",
    "\n",
    "The culprit here is `names` column, which is of `object` dtype and thus hard to store efficiently.  There are two problems here:\n",
    "\n",
    "1.  How do we store text data like `names` efficiently on disk?\n",
    "2.  Why did we have to read the `names` column when all we wanted was `amount`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Store text efficiently with categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Pandas categoricals to replace our object dtypes with a numerical representation.  This takes a bit more time up front, but results in better performance.\n",
    "\n",
    "More on categoricals at the [pandas docs](http://pandas-docs.github.io/pandas-docs-travis/categorical.html) and [this blogpost](http://matthewrocklin.com/blog/work/2015/06/18/Categoricals/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Categorize data, then store in HDFStore\n",
    "%time df.categorize(columns=['names']).to_hdf(target, '/data2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It looks the same\n",
    "df2 = dd.read_hdf(target, '/data2')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# But loads more quickly\n",
    "%time df2.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is significantly faster.\n",
    "\n",
    "This tells us that it's **not only the file type** that we use but also how we represent our variables that influences storage performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this can still be better.  \n",
    "\n",
    "* We had to read all columns (`names` and `amount`) to compute the sum of one (`amount`).  \n",
    "* We'll improve further on this with `castra`, an on-disk column-store.  \n",
    "* First though we learn about how to set an index in a `dask.dataframe`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Store columns separately with Castra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Castra`](http://github.com/Blosc/castra) is an on-disk column-store that is partitioned along the index.  It matches the `dask.dataframe` model exactly.\n",
    "\n",
    "You will likely have to install castra using `pip`.\n",
    "\n",
    "```bash\n",
    "pip install castra\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer: Castra is experimental and under heavy development churn.  You should not use it in production.*\n",
    "\n",
    "*We discuss Castra here to demonstrate the value of thinking hard about storage, not as an endorsement of its use.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.exists('tmp/accounts.castra'):\n",
    "    import shutil\n",
    "    shutil.rmtree('tmp/accounts.castra')\n",
    "\n",
    "c = df.to_castra('tmp/accounts.castra', categories=['names'])\n",
    "df3 = c.to_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time df3.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time df3.names.drop_duplicates().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Castra and partitioned indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the [previous notebook](03a-DataFrame.ipynb), `dask.dataframe` partitions your data along the index.  This can make some queries, like `loc`, `groupby`, and `join/merge` much faster because `dask.dataframe` can selectively load data and because it knows that related data is close-by.\n",
    "\n",
    "Castra uses the same model of data partitioned along the index when it stores data on disk.  By using the two together we can get efficient disk-based queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index your dataframe by `id`, then store it into a `Castra` file as we do above.  Inspect your data and verify that the index column is `id` and that it starts at `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the divisions of your dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.loc` accessor to get the records corresponding to account `100`.  Compare this to the speed you saw in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load solutions/DataFrame-02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storage choices strongly impact performance.  We evolved from text-based CSV files to binary-based Castra and saw our query times drop from 1s to 80ms.\n",
    "\n",
    "We also used `DataFrame.set_index()` to organize our data along a special column.  A common recipe for success with `dask.dataframe` is as follows:\n",
    "\n",
    "1.  Read in your data however it was delivered to you\n",
    "\n",
    "        df = dd.read_csv('myfiles.*.csv')\n",
    "    \n",
    "2.  Set your index \n",
    "\n",
    "        df2 = df.set_index('column-name')\n",
    "        \n",
    "3.  Base computation on Castra file\n",
    "\n",
    "        c = df2.to_castra('/path/to/new/file.castra', \n",
    "                          categories=['list', 'of', 'columns', 'to', 'categorize'])\n",
    "        df3 = c.to_dask()\n",
    "        \n",
    "4.  Perform efficient queries\n",
    "\n",
    "        df3.loc['2014': '2015'].groupby().events.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "*Copyright Continuum 2012-2016 All Rights Reserved.*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "continuum": {
   "depends": [
    "dask_pd"
   ],
   "requires": [
    "data/__init__.py",
    "data/solutions/*",
    "src/dask_prep.py",
    "img/hdd.jpg",
    "data/solutions/DataFrame-02.py"
   ],
   "tag": "dask_storage"
  },
  "kernelspec": {
   "display_name": "Python [conda env:iqt]",
   "language": "python",
   "name": "conda-env-iqt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
