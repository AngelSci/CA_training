{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Anaconda\"](img/anaconda-logo.png)\n",
    "<br>\n",
    "*Copyright Continuum 2012-2016 All Rights Reserved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Custom Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dask_horizontal.svg\" \n",
    "     width=\"30%\" \n",
    "     align=right\n",
    "     alt=\"Dask logo\">\n",
    "\n",
    "It is possible to submit tasks directly to the task scheduler.  This demonstrates the flexibility that can be achieved with the `submit` function and normal Python for loops.\n",
    "\n",
    "Later on we map functions across Python queues to construct data processing pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Dask Custom Workflows](#Dask-Custom-Workflows)\n",
    "* [Using Dask as a Task Scheduler](#Using-Dask-as-a-Task-Scheduler)\n",
    "\t* [Write some Code to Distribute](#Write-some-Code-to-Distribute)\n",
    "\t* [Lauch the Scheduler](#Lauch-the-Scheduler)\n",
    "\t* [Launch the Workers](#Launch-the-Workers)\n",
    "\t* [Submit a task with the Executor](#Submit-a-task-with-the-Executor)\n",
    "\t* [Submit many tasks](#Submit-many-tasks)\n",
    "\t* [Custom computation: Tree summation](#Custom-computation:-Tree-summation)\n",
    "* [Streams and Task Scheduling](#Streams-and-Task-Scheduling)\n",
    "* [Exercise: Optimization](#Exercise:-Optimization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Dask as a Task Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write some Code to Distribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def inc(x):\n",
    "    from random import random\n",
    "    sleep(random())\n",
    "    return x + 1\n",
    "\n",
    "def double(x):\n",
    "    from random import random\n",
    "    sleep(random())\n",
    "    return 2 * x\n",
    "    \n",
    "def add(x, y):\n",
    "    from random import random\n",
    "    sleep(random())\n",
    "    return x + y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lauch the Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we need to do is launch the Dask schedular on a head node.  For this local demo, the head and worker nodes will all run on the local machines, but in principle these could run on as many machines as are available for you.  Once launched, you will see terminal output similar to the below.\n",
    "\n",
    "```bash\n",
    "\n",
    "$ dask-scheduler\n",
    "\n",
    "\n",
    "distributed.scheduler - INFO - Scheduler at:         192.168.1.73:8786\n",
    "distributed.scheduler - INFO -      http at:         192.168.1.73:9786\n",
    "distributed.scheduler - INFO -  Bokeh UI at:  http://192.168.1.73:8787/status/\n",
    "bokeh.command.subcommands.serve - INFO - Check for unused sessions every 50 milliseconds\n",
    "bokeh.command.subcommands.serve - INFO - Unused sessions last for 1 milliseconds\n",
    "bokeh.command.subcommands.serve - INFO - Starting Bokeh server on port 8787 with applications at paths ['/status', '/tasks']\n",
    "distributed.core - INFO - Connection from 127.0.0.1:65440 to Scheduler\n",
    "distributed.core - INFO - Connection from 127.0.0.1:65441 to Scheduler\n",
    "distributed.core - INFO - Connection from 127.0.0.1:50703 to Scheduler\n",
    "distributed.scheduler - INFO - Register 127.0.0.1:50701\n",
    "distributed.scheduler - INFO - Starting worker compute stream, 127.0.0.1:50701\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the Workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the scheduler is launched, we want to start one or more workers by connecting them to the scheduler.  In this case we connect to `localhost` on the default port of 8786.  For the cluster scenario, copy the address echoed when you launched `dask-scheduler`; in this case, you could use `192.168.1.73:8786`.  See `dask-worker --help` for more options.  In particular, you may want to fine-tune the number of processes and/or threads devoted to participating in the Dask-Distributed work (this is not to limit resource allocation, but rather to take best advantage of the cores on your nodes; the defaults are sensible most of the time).\n",
    "\n",
    "```bash\n",
    "\n",
    "$ dask-worker localhost:8786\n",
    "\n",
    "\n",
    "distributed.worker - INFO -       Start worker at:            127.0.0.1:50701\n",
    "distributed.worker - INFO -              nanny at:            127.0.0.1:50698\n",
    "distributed.worker - INFO -               http at:            127.0.0.1:50700\n",
    "distributed.worker - INFO - Waiting to connect to:            127.0.0.1:8786\n",
    "distributed.worker - INFO -         Registered to:            127.0.0.1:8786\n",
    "distributed.core - INFO - Connection from 127.0.0.1:50704 to Worker\n",
    "distributed.core - INFO - Connection from 127.0.0.1:50705 to Worker\n",
    "distributed.nanny - INFO - Nanny 127.0.0.1:50698 starts worker process 127.0.0.1:50701\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit a task with the Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Executor, progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Depending on platform, might need to launch using `127.0.0.1` rather than `localhost`\n",
    "\n",
    "#e = Executor('52.91.11.18:8786')  # We might connect to a remote cluster\n",
    "e = Executor('127.0.0.1:8786')\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can execute the `inc()` function locally to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inc(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the job instead to the executor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Note: Before issuing the `.submit()`, if you open a browser window/tab pointed to http://localhost:8787/status/, you get a very nice status monitor during the task execution.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future = e.submit(inc, 1)  # returns immediately with pending future\n",
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future  # scheduler and client talk constantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit many tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We submit many tasks that depend on each other in a normal Python for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "zs = []\n",
    "for i in range(16):\n",
    "    x = e.submit(inc, i)     # x = inc(i)\n",
    "    y = e.submit(double, x)  # y = inc(x)\n",
    "    z = e.submit(add, x, y)  # z = inc(y)\n",
    "    zs.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e.gather(zs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom computation: Tree summation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of a non-trivial algorithm, consider the classic tree reduction.  We accomplish this with a nested for loop and a bit of normal Python logic.\n",
    "\n",
    "```\n",
    "finish           total             single output\n",
    "    ^          /        \\\n",
    "    |        c1          c2        neighbors merge\n",
    "    |       /  \\        /  \\\n",
    "    |     b1    b2    b3    b4     neighbors merge\n",
    "    ^    / \\   / \\   / \\   / \\\n",
    "start   a1 a2 a3 a4 a5 a6 a7 a8    many inputs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = zs\n",
    "while len(L) > 1:\n",
    "    new_L = []\n",
    "    for i in range(0, len(L), 2):\n",
    "        future = e.submit(add, L[i], L[i + 1])  # add neighbors\n",
    "        new_L.append(future)\n",
    "    L = new_L                                   # swap old list for new\n",
    "    \n",
    "progress(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e.gather(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streams and Task Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The executor can map functions over lists or queues.  \n",
    "* This is nothing more than calling `submit` many times.  \n",
    "* We can chain maps on queues together to construct simple data processing pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "All of this logic happens on the client-side.  \n",
    "* None of this logic was hard-coded into the scheduler.  \n",
    "* This simple streaming system is a good example of the kind of system that becomes easy for users to build when given access to custom task scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "def multiplex(n, q, **kwargs):\n",
    "    \"\"\" Convert one queue into several equivalent Queues\n",
    "    \n",
    "    >>> q1, q2, q3 = multiplex(3, in_q)\n",
    "    \"\"\"\n",
    "    out_queues = [Queue(**kwargs) for i in range(n)]\n",
    "    def f():\n",
    "        while True:\n",
    "            x = q.get()\n",
    "            for out_q in out_queues:\n",
    "                out_q.put(x)\n",
    "    t = Thread(target=f)\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "    return out_queues        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "           ----inc---->\n",
    "          /            \\ \n",
    "in_q --> q              \\_add__ results\n",
    "          \\             / \n",
    "           ---double-->/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_q = Queue()\n",
    "q = e.scatter(in_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_q.put(1)\n",
    "q.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_1, q_2 = multiplex(2, q)\n",
    "\n",
    "inc_q = e.map(inc, q_1)\n",
    "double_q = e.map(double, q_2)\n",
    "\n",
    "add_q = e.map(add, inc_q, double_q)\n",
    "\n",
    "out_q = e.gather(add_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_q.put(10)\n",
    "out_q.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "def feed(q):\n",
    "    for i in range(10000):\n",
    "        sleep(random())\n",
    "        q.put(i)\n",
    "        \n",
    "t = Thread(target=feed, args=(q,))\n",
    "t.daemon = True\n",
    "t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_q.qsize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Identify a more realistically difficult computation that is parallelizable.  Play around with parallelizing it over your machine cores (or over a cluster if one is available in class).\n",
    "\n",
    "2. Try using Numba JITing to further speed up this distributed computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright Continuum 2012-2016 All Rights Reserved.*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:iqt]",
   "language": "python",
   "name": "conda-env-iqt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
