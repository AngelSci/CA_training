{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/anaconda-logo.png' align='left' style=\"padding:10px\">\n",
    "<br>\n",
    "*Copyright Continuum 2012-2016 All Rights Reserved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerate Natural Language Processing: LDA Topic Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a unsupervised learning algorithm to extract topics from documents.  A trained LDA model can transform documents into the semantic space, a vector describing how likely a document is of a certain topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [LDA Topic Clustering](#LDA-Topic-Clustering)\n",
    "\t* [Load data](#Load-data)\n",
    "\t* [Build dictionary](#Build-dictionary)\n",
    "\t* [Build corpus](#Build-corpus)\n",
    "\t* [Training](#Training)\n",
    "\t* [Find topic from documents](#Find-topic-from-documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Example using GenSim's LDA and sklearn. \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# Accelerated gensim version, LdaJitModel is not in the original distribution\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models import LdaModel, LdaJitModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_20newsgroups_ dataset\n",
    "\n",
    "See http://scikit-learn.org/stable/datasets/twenty_newsgroups.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rand = np.random.mtrand.RandomState(8675309)  # set random seed for better reproducibility\n",
    "\n",
    "cats = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "traindata = fetch_20newsgroups(subset='train',                  # using the training set\n",
    "                          categories=cats,                      # four different categories\n",
    "                          shuffle=True,                         # shuffle the data\n",
    "                          remove=('headers', 'footers', 'quotes'), # clean the data\n",
    "                          random_state=rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four very different topics are selected so that we can easily see the expected result with shorter training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('number of documents', len(traindata.data))\n",
    "print('number of characters', sum(len(d) for d in traindata.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokenize and preprocess the documents\n",
    "    * normalize the words and remove stopwords\n",
    "* Build dictionary\n",
    "* Filter out words that are infrequent (not enough information) and too frequent (probably meaningless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "id2word = Dictionary(map(tokenize, traindata.data))\n",
    "print(id2word)\n",
    "\n",
    "# filter out words that are infrequent and too frequent\n",
    "id2word.filter_extremes(no_below=10, no_above=0.97)\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(tokenize(doc)) for doc in traindata.data]\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the standard model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit LDA.\n",
    "lda = LdaModel(corpus, id2word=id2word, num_topics=5, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster training time with ``LdaJitModel``, an optimized version of ``LdaModel`` by speeding up critical components of training procedure using Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit LDA.\n",
    "lda_jit = LdaJitModel(corpus, id2word=id2word, num_topics=5, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_jit.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** due to randomness in the training and the low number of passes, the topics may not match exactly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for real (more passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit LDA.\n",
    "lda_jit = LdaJitModel(corpus, id2word=id2word, num_topics=5, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_jit.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find topic from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdata = fetch_20newsgroups(subset='test',  # now switching to the test dataset\n",
    "                              categories=cats,\n",
    "                              shuffle=True,\n",
    "                              remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = 2\n",
    "doc = testdata.data[idx]\n",
    "print('expected topic:\\n', testdata.target_names[testdata.target[idx]])\n",
    "print('content:\\n', doc[:1000])\n",
    "\n",
    "# create bag-of-words\n",
    "bow = id2word.doc2bow(tokenize(doc))\n",
    "# transform to semantic space\n",
    "vector = lda[bow]\n",
    "# get best topic\n",
    "best_topicid = max(vector, key=lambda x: abs(x[1]))[0]\n",
    "lda.show_topic(best_topicid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign topics for all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "doc_topics = defaultdict(list)\n",
    "\n",
    "for docid, doc in enumerate(testdata.data):\n",
    "    bow = id2word.doc2bow(tokenize(doc))\n",
    "    if bow:  # if not empty doc\n",
    "        # Get vector in semantic space.\n",
    "        # Each dimension corresponds to topic.\n",
    "        vector = lda[bow]   \n",
    "        # Use the \"strongest\" topic as the representing topic\n",
    "        topicid = max(vector, key=lambda x: abs(x[1]))[0]\n",
    "        doc_topics[topicid].append(docid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print assigned topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for topicid, documents in doc_topics.items():\n",
    "    print('=' * 80)\n",
    "    print(\"Inferred Topic Terms:\", lda.print_topic(topicid, topn=5))\n",
    "    \n",
    "    for i in range(3):\n",
    "        print(str(i).center(80, '-'))\n",
    "        docid = documents[i]\n",
    "        print(\"Expected Category:\", testdata.target_names[testdata.target[docid]])\n",
    "        print(\"Document:\")\n",
    "        print(testdata.data[docid].lstrip()[:500])\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Copyright Continuum 2012-2016 All Rights Reserved.*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:iqt]",
   "language": "python",
   "name": "conda-env-iqt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
