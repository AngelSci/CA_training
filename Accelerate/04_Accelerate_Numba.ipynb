{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/anaconda-logo.png' align='left' style=\"padding:10px\">\n",
    "<br>\n",
    "*Copyright Continuum 2012-2016 All Rights Reserved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerate: Faster Arrays with Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba can speed up your applications with high performance functions written directly in Python. \n",
    "\n",
    "With a few simple annotations, array-oriented computationally-intensive Python code can be optimized just-in-time to perform as well as, and sometimes better than, pre-compiled C, C++ and Fortran code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* intended to accelerate mathematical and scientific Python code.\n",
    "* integration with the Python scientific software stack (thanks to Numpy)\n",
    "* on-the-fly code generation (at import time or runtime, at the userâ€™s preference)\n",
    "* native code generation for the CPU (default) and GPU hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Accelerate: Faster Arrays with Numba](#Accelerate:-Faster-Arrays-with-Numba)\n",
    "\t* [Numba Overview](#Numba-Overview)\n",
    "\t* [Numba Features](#Numba-Features)\n",
    "\t* [Numba Set-up](#Numba-Set-up)\n",
    "* [Why is Numba Needed?](#Why-is-Numba-Needed?)\n",
    "\t* [Python, Numpy, and Memory](#Python,-Numpy,-and-Memory)\n",
    "\t* [Numba and Memory](#Numba-and-Memory)\n",
    "* [Numba JIT](#Numba-JIT)\n",
    "\t* [Numba ``@jit`` decorator](#Numba-@jit-decorator)\n",
    "\t* [Exercise: JIT a for loop](#Exercise:-JIT-a-for-loop)\n",
    "\t* [Numba JIT: Example: 2D Sum](#Numba-JIT:-Example:-2D-Sum)\n",
    "\t* [Numba JIT Example: Cumulative Sum](#Numba-JIT-Example:-Cumulative-Sum)\n",
    "\t* [Exercise: Compute $\\pi$ Faster](#Exercise:-Compute-$\\pi$-Faster)\n",
    "\t* [What is Numba Doing? LLVM and JIT Compilation](#What-is-Numba-Doing?-LLVM-and-JIT-Compilation)\n",
    "\t* [Inspecting LLVM and JIT Outputs](#Inspecting-LLVM-and-JIT-Outputs)\n",
    "* [Numba Vectorize](#Numba-Vectorize)\n",
    "\t* [Numpy Ufuncs](#Numpy-Ufuncs)\n",
    "\t* [Numpy Example: Computing a Signal](#Numpy-Example:-Computing-a-Signal)\n",
    "\t* [NumPy Broadcasting](#NumPy-Broadcasting)\n",
    "\t* [Creating Ufuncs with Numba](#Creating-Ufuncs-with-Numba)\n",
    "\t* [Exercise: Vectorize the Signal](#Exercise:-Vectorize-the-Signal)\n",
    "* [Numba and GPUs](#Numba-and-GPUs)\n",
    "\t* [Introduction to CUDA](#Introduction-to-CUDA)\n",
    "\t* [Set-up and Test](#Set-up-and-Test)\n",
    "\t* [CUDA and Ufuncs](#CUDA-and-Ufuncs)\n",
    "\t* [CUDA and Trigonometry](#CUDA-and-Trigonometry)\n",
    "* [Numba Strategies](#Numba-Strategies)\n",
    "\t* [Math and Science](#Math-and-Science)\n",
    "\t* [Specify Types](#Specify-Types)\n",
    "\t* [Targeted Optimization](#Targeted-Optimization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit\n",
    "from numba import vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is Numba Needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computationally intensive operations usually involve...\n",
    "\n",
    "* arrays and loops\n",
    "* same operation is applied to a large number of data elements\n",
    "* elements stored in an array or container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python, Numpy, and Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeated access to array element within a loop can be expensive due to the inefficiency of the interpreted code.\n",
    "\n",
    "* The most popular array container in python is the NumPy ndarray\n",
    "* n-dimensional array object with data stored in a single memory buffer.\n",
    "* In python, access to the memory buffer of a ndarray is inefficient.\n",
    "* The interpreter must go through layers of methods due to indirection and finally into C code that directly reads the memory.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where Numba comes in ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba and Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* uses a LLVM as a JIT compiler, emits machine code\n",
    "* direct access the underlying memory buffer\n",
    "* elminates the inefficiency in the interpreted code\n",
    "* resulting code can perform as fast as the equivalent C code \n",
    "* retains the flexibility of high-level python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba JIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba ``@jit`` decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba provides just-in-time (JIT) compiling via a function decorator `@jit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def add(a,b):\n",
    "    c = a + b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit add(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: JIT a for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that sums all the integers from 1 to n, using a for loop.\n",
    "\n",
    "* add an input parameter that determines the number of iterations `n` in the for loop\n",
    "* first implementation, do ***not** use `@jit`\n",
    "* second version, add the `@jit` decorator\n",
    "* use `%timeit` for compare performance of both implementations with `n=10` iterations\n",
    "* use `%timeit` for compare performance of both implementations with `n=1000000` iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func1(n=10):\n",
    "    total = 0\n",
    "    for item in range(n):\n",
    "        total+=item\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def func2(n=10):\n",
    "    total = 0\n",
    "    for item in range(n):\n",
    "        total+=item\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit func1(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit func2(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit func1(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit func2(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using pure python `for` loops is almost always slower than any other implementation. Use numpy or numba jit when you can. But which is better, numpy or numba?...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: JIT a 2D Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement 4 different implementations of a 2-dimnesional sum\n",
    "* python\n",
    "* python + numba `@jit`\n",
    "* python + numba `@jit` + input type specification\n",
    "* numpy\n",
    "\n",
    "We will time each and compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure python implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum2d(arr):\n",
    "    M, N = arr.shape\n",
    "    total = 0.0\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            total += arr[i,j]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure python with numba `@jit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def sum2d_jit(arr):\n",
    "    M, N = arr.shape\n",
    "    total = 0.0\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            total += arr[i,j]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure python with numba `@jit`, and with type specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@jit('float32(float32[:])')\n",
    "def sum2d_jit_typed(arr):\n",
    "    M, N = arr.shape\n",
    "    total = 0.0\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            total += arr[i,j]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a numpy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum2d_numpy(arr):\n",
    "    M, N = arr.shape\n",
    "    total = arr.sum()\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the timing comparisons..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim_size = 1000\n",
    "arr = np.random.random((dim_size,dim_size))\n",
    "\n",
    "print(\"\\n Timing the non-numba run\")\n",
    "%timeit sum2d(arr)\n",
    "print(\"\\n Timing the numba run\")\n",
    "%timeit sum2d_jit(arr)\n",
    "print(\"\\n Timing the numba typed run\")\n",
    "%timeit sum2d_jit_typed(arr)\n",
    "print(\"\\n Timing the numpy run\")\n",
    "%timeit sum2d_numpy(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Numba is not always the best answer. You have to try to find out.***\n",
    "\n",
    "* Numba provided an improvement over pure python, but in this case, is still slower than numpy. \n",
    "* In the next example, the nature of the computation will benefit more from numba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: JIT a Cumulative Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a different problem and see how numba performs in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement cumulative sum of an array.  (aka. inclusive-scan)\n",
    "\n",
    "$$ y_i = \\sum^{i}_{j=0}{x_j}  $$\n",
    "\n",
    "Every element of the output is the sum of all previous elements in the input including the element at the current index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, our implementations will be:\n",
    "\n",
    "* python\n",
    "* numpy\n",
    "* python with ``@jit``\n",
    "* numpy with ``@jit``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define the python implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cumsum(arr):\n",
    "    \"Perform a cummulative reduction over addition\"\n",
    "    assert arr.ndim == 1\n",
    "    accum = 0                      # accumulator (identity on domain)\n",
    "    out = np.zeros_like(arr)       # allocate output array\n",
    "    for i in range(arr.shape[0]):  # loop over every element\n",
    "        accum += arr[i]            # accumulate values from the input \n",
    "        out[i] = accum             # store the accumulator to the current output\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply ``@jit`` to the our pure python implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "# Identical code as above, just showing use of decorator\n",
    "# We could equivalently write `fast_accumulate = jit(accumulate)`\n",
    "@jit\n",
    "def cumsum_jit(arr):             \n",
    "    \"Perform a cummulative reduction over addition\"\n",
    "    assert arr.ndim == 1\n",
    "    accum = 0                      # accumulator (identity on domain)\n",
    "    out = np.zeros_like(arr)       # allocate output array\n",
    "    for i in range(arr.shape[0]):  # loop over every element\n",
    "        accum += arr[i]            # accumulate values from the input \n",
    "        out[i] = accum             # store the accumulator to the current output\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining our pure numpy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cumsum_np(arr):\n",
    "    return np.cumsum(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our numpy with `@jit` implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def cumsum_np_jit(arr):\n",
    "    return np.cumsum(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the numerical outputs of all implementations to verify that they match the expected numerical outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = np.random.randint(1, 4, 10)      # test with random array\n",
    "print('arr', arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expected = np.cumsum(arr)\n",
    "out_py     = cumsum(arr)\n",
    "out_py_jit = cumsum_jit(arr)\n",
    "out_np     = cumsum_np(arr)\n",
    "out_np_jit = cumsum_np_jit(arr)\n",
    "\n",
    "print('out_py',     out_py)\n",
    "print('out_py_jit', out_py_jit)\n",
    "print('out_np',     out_np)\n",
    "print('out_np_jit', out_np_jit)\n",
    "\n",
    "assert np.all(out_py == expected)\n",
    "assert np.all(out_py_jit == expected)\n",
    "assert np.all(out_np_jit == expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the speed of the 4 implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = np.random.randint(1, 4, 1000)\n",
    "\n",
    "print(\"\\n Python\")\n",
    "%timeit cumsum(arr)\n",
    "print(\"\\n NumPy\")\n",
    "%timeit cumsum_np(arr)\n",
    "print(\"\\n Python with JIT\")\n",
    "%timeit cumsum_jit(arr)\n",
    "print(\"\\n Numpy with JIT\")\n",
    "%timeit cumsum_np_jit(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, numba wins. In many cases, it helps to test different implementations such as the examples given above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what changed? How do you decide when to `@jit` and when not to `@jit`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Compute $\\pi$ Faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we previously used the Accelerate profiler to look at different implementations of the [Wallis product](https://en.wikipedia.org/wiki/Wallis_product) for estimating the value of $\\pi$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 1655, John Wallis determined that $\\pi$ could be computed as a product of ratios:\n",
    "\n",
    "$$\\pi = 2\\prod_{i=1}^{\\infty}\\frac{4i^2}{4i^2-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Numba two develop two additional implementations, with python and numpy, and then compare all 4 with the Acccelerate profiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python implementation\n",
    "def compute_pi_v1(n):\n",
    "    pi = 2.0\n",
    "    for i in range(1,n):\n",
    "        tmp = 4*i**2\n",
    "        pi *= tmp/(tmp-1)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy implementation\n",
    "def compute_pi_v2(n):\n",
    "    series = 4.0*np.arange(1,n)**2\n",
    "    series /= (series-1)\n",
    "    return 2.0*series.prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Python and @jit\n",
    "\n",
    "@jit\n",
    "def compute_pi_v3(n):\n",
    "    pi = 2.0\n",
    "    for i in range(1,n):\n",
    "        tmp = 4*i**2\n",
    "        pi *= tmp/(tmp-1)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy and @jit\n",
    "\n",
    "@jit\n",
    "def compute_pi_v4(n):\n",
    "    series = 4.0*np.arange(1,n)**2\n",
    "    series /= (series-1)\n",
    "    return 2.0*series.prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will perform `%timeit` profiling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = int(1e6)\n",
    "print('Version 1 Profiled')\n",
    "%timeit compute_pi_v1(n)\n",
    "print('Version 2 Profiled')\n",
    "%timeit compute_pi_v2(n)\n",
    "print('Version 3 Profiled')\n",
    "%timeit compute_pi_v3(n)\n",
    "print('Version 4 Profiled')\n",
    "%timeit compute_pi_v4(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the Accelerate profiler to compare all 4 implementations:\n",
    "from accelerate import profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p1 = profiler.Profile()\n",
    "p1.enable()\n",
    "compute_pi_v1(n)\n",
    "p1.disable()\n",
    "p1.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p2 = profiler.Profile()\n",
    "p2.enable()\n",
    "compute_pi_v2(n)\n",
    "p2.disable()\n",
    "p2.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p3 = profiler.Profile()\n",
    "p3.enable()\n",
    "compute_pi_v3(n)\n",
    "p3.disable()\n",
    "p3.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p4 = profiler.Profile()\n",
    "p4.enable()\n",
    "compute_pi_v4(n)\n",
    "p4.disable()\n",
    "p4.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Numba Doing? LLVM and JIT Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some understanding of how numba works can help select the best strategies to test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/numba-llvm.png\" width=\"45%\" align=\"right\"/>\n",
    "---\n",
    "\n",
    "**Numba works by compilation in two stages**:\n",
    "* Numba converts python to LLVM (low-level virtual machine) code\n",
    "* LLVM JIT compiler converts LLVM code to machine-specific assemply code\n",
    "\n",
    "**Numba JIT'ed code can be faster than precompiled C code**\n",
    "* the LLVM JIT compilier can emit specialized instructions for the specific host CPU.\n",
    "* versus precompiled code, which uses generic instructions for maximum portability.\n",
    "* JIT'ed code paths are also specialized over argument types to get maximum benefit from JIT'ing.\n",
    "\n",
    "**Numba often (not always) improves code already written using NumPy**.\n",
    "* The array type in NumPy is accessible to Numba's intermediate representation (IR)\n",
    "* Often more specific datatypes or code paths can be identified than those in the general ufuncs of NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting LLVM and JIT Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compilation Step 1**: It is posisble to inspect the LLVM code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, v in cumsum_np_jit.inspect_llvm().items():  # loop through each overload\n",
    "    print('signature', k)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compilation Step 2**: It is even possible to inspect the JIT-generated assembly code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, v in cumsum_np_jit.inspect_asm().items():  # loop through each overload\n",
    "    print('signature', k)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba Vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create your own vectorized universal functions with numba!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Numpy Ufuncs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most ***numpy*** array operations are **implemented as ufuncs** (*\"universal functions\"*). \n",
    "\n",
    "These are flexible functions that operate on arrays with compatible shapes, performing element-by-element calculations in a vectorized fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Example: Computing a Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Amplitudes, Frequencies, Phases, and times, for two components of a signal\n",
    "a1 = 1.0\n",
    "a2 = 3.0\n",
    "f1 = 2.0\n",
    "f2 = 4.0\n",
    "p1 = 0\n",
    "p2 = np.pi*np.random.random()\n",
    "num_times = 10001\n",
    "t = np.linspace(-2*np.pi, +2*np.pi, num_times)\n",
    "\n",
    "# vectorized operations on numpy arrays with numpy ufunc operators {*, +, /}\n",
    "t1 = t*(2*np.pi/f1) + p1\n",
    "t2 = t*(2*np.pi/f2) + p2\n",
    "\n",
    "# adding some noise to the amplitudes\n",
    "n1 = 1 + 0.2*np.random.random(num_times)\n",
    "n2 = 1 + 0.2*np.random.random(num_times)\n",
    "\n",
    "# vectorized operations on numpy arrays with numpy ufunc functions {sin(), cos()}\n",
    "\n",
    "# computing the signal component\n",
    "s1 = a1*n1*np.sin(t1)\n",
    "s2 = a2*n2*np.cos(t2)\n",
    "\n",
    "# computing the total signal\n",
    "y  = s1*s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the result\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t,y)\n",
    "ax.set_xlim(-2*np.pi, +2*np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy uses a _**broadcasting**_ rule to adjust the shapes and dimenions of the operands.\n",
    "* Broadcasting is a pairwise operation on the shapes of the arguments.\n",
    "* Its result is a new shape that is compatible with both arguments.\n",
    "* Each value in the shape corresponds to the size of a dimension.\n",
    "* Any dimension that is sized 0 or 1 can be raised to any larger dimension by repeating the values in inner dimensions.\n",
    "* By treating scalars as 0-dimension arrays, scalar can be broadcasted to arrays as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Ufuncs with Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example the function $y' = ax + y$\n",
    "is implemented using ufuncs (i.e. arithmetic operations on numpy arrays are implemented via ufuncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.random.random(10)\n",
    "x = np.random.random(10)\n",
    "y = np.random.random(10)\n",
    "\n",
    "yprime = a * x + y\n",
    "print(yprime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba can JIT array expressions like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def axpy(a, x, y):\n",
    "    return a * x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yprime_numba = axpy(a, x, y)\n",
    "print(yprime_numba)\n",
    "assert np.all(yprime_numba == yprime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numba can also generate ufuncs directly.** This is accomplished using the **``@vectorize``** decorator.\n",
    "\n",
    "The function being \"vectorized\" is the kernel, a scalar function that is applied element-wise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify signatures to compile.  i.e. single and double precision versions\n",
    "signatures = ['(float32, float32, float32)', \n",
    "              '(float64, float64, float64)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@vectorize(signatures)\n",
    "def axpy_ufunc(a, x, y):\n",
    "    # This function receives scalar arguments\n",
    "    return a * x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yprime_ufunc = axpy_ufunc(a, x, y)\n",
    "print(yprime_ufunc)\n",
    "assert np.all(yprime_ufunc == yprime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba ufuncs are powerful because they can **target multicore execution** (this example) and **GPU execution** (next section below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@vectorize(signatures, target='parallel')  # CPU threads\n",
    "def axpy_ufunc_par(a, x, y):\n",
    "    return a * x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yprime_ufunc_par = axpy_ufunc_par(a, x, y)\n",
    "print(yprime_ufunc_par)\n",
    "assert np.all(yprime_ufunc_par == yprime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profiling them shows the relative performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 10 ** 7\n",
    "a = np.random.random(n)\n",
    "x = np.random.random(n)\n",
    "y = np.random.random(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('NumPy array expresion')\n",
    "%timeit a * x + y\n",
    "print('\\nNumba jit')\n",
    "%timeit axpy(a, x, y)\n",
    "print('\\nNumba vectorize serial')\n",
    "%timeit axpy_ufunc(a, x, y)\n",
    "print('\\nNumba vectorize multithreaded')\n",
    "%timeit axpy_ufunc_par(a, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Vectorize the Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous example, we constructed a signal using numpy trigonometry functions. \n",
    "\n",
    "* Create a function called `signal_numpy()` using the code from the numpy example above.\n",
    "* Create a function called `signal_math()` that does the same thing, but using `math.sin()` and `math.cos()` instead of the numpy functions.\n",
    "* Create a fucntion called `signal_math_ufunc()` that uses `signal_math()` and `@vectorize` to create a `ufunc`\n",
    "* Use `%timeit` or another profiler to compare run time performance of the three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All implementations should take the following as input parameters, with the defaults shown below:\n",
    "\n",
    "```\n",
    "a1 = 1.0\n",
    "a2 = 3.0\n",
    "f1 = 2.0\n",
    "f2 = 4.0\n",
    "p1 = 0\n",
    "p2 = np.pi\n",
    "num_times = 10001\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint: it's okay with use tuple unpacking, i.e. construct a dictionary of keyword-args and pass in `**kwargs`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def signal_math():\n",
    "    # code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decorator here\n",
    "def signal_math_ufunc():\n",
    "    # code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def signal_numpy():\n",
    "    # code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Profile run times and compare all three here. \n",
    "# How do you think signal_math_ufunc() will compare with signal_numpy()?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Numba contains support for CUDA GPU programming. \n",
    "* Numba provides a Python dialect for low-level programming on the CUDA GPU hardware. \n",
    "* It provides full control over the hardware for fine tunning the performance of CUDA kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**: \n",
    "http://numba.pydata.org/numba-doc/0.13/CUDAintro.html\n",
    "\n",
    "> *A CUDA GPU contains one or more streaming multiprocessors (SMs). Each SM is a many-core processor that is optimized for high throughput. The manycore architecture is very different from the common multicore CPU architecture. Instead of having a large cache and complex logic for instruction level optimization, a manycore processor achieves high throughput by executing many threads in parallel on many simpler cores. It overcomes latency due to cache miss or long operations by using zero-cost context switching. It is common to launch a CUDA kernel with hundreds or thousands of threads to keep the GPU busy.*\n",
    "\n",
    "> *The CUDA programming model is similar to the SIMD vector model in modern CPUs. A CUDA SM schedules the same instruction from a warp of 32-threads at each issuing cycle. The advantage of CUDA is that the programmer does not need to handle the divergence of execution path in a warp, whereas a SIMD programmer would be required to properly mask and shuffle the vectors. The CUDA model decouples the data structure from the program logic.*\n",
    "\n",
    "To know more about CUDA, please refer to NVIDIA CUDA-C Programming Guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples in this section of the lesson will not run if your computer does not have a CUDA compatible GPU.\n",
    "\n",
    "* the notes below outline the requirements for the CUDA examples\n",
    "* there is a cuda support test below which will bypass CUDA examples if your hardware cannot run them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirements**\n",
    "\n",
    "* A CUDA-Enabled GPU: http://numba.pydata.org/numba-doc/0.13/CUDASupport.html\n",
    "* set the path `NUMBAPRO_CUDA_DRIVER` to your CUDA driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following error message with occur if you have not set the driver path:\n",
    "\n",
    "> ```\n",
    "CUDA driver library cannot be found.\n",
    "If you are sure that a CUDA driver is installed,\n",
    "try setting environment variable NUMBAPRO_CUDA_DRIVER\n",
    "with the file path of the CUDA driver shared library.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your system for CUDA support by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    from numba import cuda\n",
    "    cuda.detect()\n",
    "    assert cuda.is_available()\n",
    "    cuda_capable = True\n",
    "except Exception as e:\n",
    "    e = sys.exc_info()[0]\n",
    "    print(  \"Error: %s\" % str(e) )\n",
    "    cuda_capable = False\n",
    "print( \"CUDA capable =\", cuda_capable )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA and Ufuncs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioend above, Numba created ufuncs can be targeted for `parallel` or `cuda` execution.\n",
    "\n",
    "Below are mutiple implementations we will profile using a numpy implementation as our baseline for performance comparisons.\n",
    "\n",
    "* Numpy\n",
    "* Numba JIT\n",
    "* Numba Vectorize (serial)\n",
    "* Numba Vectorize (parallel, mutithreaded on CPU)\n",
    "* Numba Vectorize (parallel, on CUDA GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy Ufunc\n",
    "def axpy_numpy_ufunc(a, x, y):\n",
    "    return a * x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NUMBA JIT\n",
    "@jit\n",
    "def axpy_numba_jit(a, x, y):\n",
    "    return a * x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NUMBA Ufunc\n",
    "@vectorize(signatures)\n",
    "def axpy_numba_vectorize(a, x, y):\n",
    "    return a * x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NUMBA Ufunc targeted at CPU threads\n",
    "@vectorize(signatures, target='parallel')\n",
    "def axpy_numba_parallel(a, x, y):\n",
    "    return a * x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NUMBA Ufunc targeted at GPU execution\n",
    "@vectorize(signatures, target='cuda')\n",
    "def axpy_numba_cuda(a, x, y):\n",
    "    return a * x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 10 ** 7\n",
    "a = np.random.random(n)\n",
    "x = np.random.random(n)\n",
    "y = np.random.random(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('NumPy array expresion')\n",
    "%timeit axpy_numpy_ufunc(a, x, y)\n",
    "print('\\nNumba jit')\n",
    "%timeit axpy_numba_jit(a, x, y)\n",
    "print('\\nNumba vectorize serial')\n",
    "%timeit axpy_numba_vectorize(a, x, y)\n",
    "print('\\nNumba vectorize parallel/multithreaded')\n",
    "%timeit axpy_numba_parallel(a, x, y)\n",
    "print('\\nNumba vectorize CUDA/GPU')\n",
    "if cuda_capable:\n",
    "    %timeit axpy_numba_cuda(a, x, y)\n",
    "else:\n",
    "    print('\\nSystem not CUDA capable. Did not run.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA and Trigonometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates when the GPU ufunc can speed things up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU has dedicated special function units for computing transcendental functions like sin and cosine.  \n",
    "\n",
    "These operations can be a lot faster on the GPU even if the data has to be transferred between the CPU and GPU via the PCI-express."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def trig(x, y):\n",
    "    return math.sin(x) + math.cos(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function signatures (input and return types) to compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trig_sig = ['float32(float32, float32)', 'float64(float64, float64)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the different implementations to compare when profiling performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specialize for multicore CPU version\n",
    "trig_par = vectorize(trig_sig, target='parallel')(trig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specialize for CUDA version\n",
    "trig_gpu = vectorize(trig_sig, target='cuda')(trig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specialize for default serial CPU version\n",
    "trig_serial = vectorize(trig_sig)(trig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define input and test numerical output for GPU implementation aginst a trusted baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 10 ** 7\n",
    "x = np.random.random(n).astype(np.float32).reshape((100,-1))\n",
    "y = np.random.random(n).astype(np.float32).reshape((100,-1))\n",
    "\n",
    "assert np.allclose(np.sin(x) + np.cos(y), trig_gpu(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profile all implementation and compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('NumPy trig')\n",
    "%timeit np.sin(x) + np.cos(y)\n",
    "print('\\nNumba ufunc serial')\n",
    "%timeit trig_serial(x, y)\n",
    "print('\\nNumba ufunc multithread')\n",
    "%timeit trig_par(x, y)\n",
    "print('\\nNumba ufunc gpu')\n",
    "if cuda_capable:\n",
    "    %timeit trig_gpu(x, y)\n",
    "else:\n",
    "    print('\\nSystem not CUDA capable. Did not run.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the general advice regarding optimization still applies. However, here are some Numba-specific strategies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math and Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Numba is largely intended to accelerate mathematical and scientific Python code\n",
    "* Applying numba to very short, trivial functions or arbitrary python objects will not likely help.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If the function is only called once, specify the types so that the function will be pre-compiled.\n",
    "* If a type signature is not specified, Numba will guess datatypes and specialize the function when it is first executed.\n",
    "* This means that the first execution will be slow because numba has to compile it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targeted Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Only try to compile the critical paths in your code\n",
    "* If you have a piece of performance-critical computational code amongst some higher-level code, refactor\n",
    "* Factoring out the performance-critical code in a separate function allows you to compile just that function with Numba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Copyright Continuum 2012-2016 All Rights Reserved.*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:iqt]",
   "language": "python",
   "name": "conda-env-iqt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
