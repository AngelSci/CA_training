{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/anaconda-logo.png' align='left' style=\"padding:10px\">\n",
    "<br>\n",
    "*Copyright Continuum 2012-2016 All Rights Reserved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerate GPU Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Both **Numba and Accelerate** provide CUDA programming capability\n",
    "* **Numba** focuses on the ability to create **custom CUDA functions** and misc CUDA operations\n",
    "    * (i.e. memory allocation and transfer between CPU and GPU)\n",
    "* **Accelerate** focuses on **pre-built CUDA functions** and CUDA library bindings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Accelerate GPU Programming](#Accelerate-GPU-Programming)\n",
    "* [Memory](#Memory)\n",
    "\t* [Numpy arrays in device memory](#Numpy-arrays-in-device-memory)\n",
    "* [Ufuncs](#Ufuncs)\n",
    "* [Writing custom CUDA functions](#Writing-custom-CUDA-functions)\n",
    "\t* [Thread Hierarchy](#Thread-Hierarchy)\n",
    "\t* [Memory Hierarchy](#Memory-Hierarchy)\n",
    "\t* [Exercise: Discard unneeded intermediate reductions](#Exercise:-Discard-unneeded-intermediate-reductions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first major difference between CPU and GPU programming is the memory.\n",
    "\n",
    "* Each GPU has its own memory, referred to here as the ***device memory***.\n",
    "    * The device memory is on the GPU card.\n",
    "    * It is separated from the ***host*** memory by the PCI-express bus.\n",
    "    * The ***device (GPU)*** cannot access ***host*** memory allocated normally.  \n",
    "* The CPU has access to the ***host memory***.\n",
    "    * The ***host memory*** is the system primary memory (i.e. RAM).\n",
    "    * The **host*** (CPU) cannot directly access ***device memory***.\n",
    "\n",
    "***Note: CUDA driver provides special allocators to create page-locked host memory for access from device memory.  But that is not common, and is beyound the scope of this course.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy arrays in device memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA functions in Numba and Accelerate can use Numpy arrays as arguments by implicitly transfering the array to device memory.  For performance reason, it is sometimes preferred to do explicit memory transfer.\n",
    "\n",
    "To copy an array to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "arr = np.random.random(10)   # init random array on the host\n",
    "d_arr = cuda.to_device(arr)  # copy to device\n",
    "\n",
    "print(arr, arr.shape, arr.dtype)\n",
    "print(d_arr, d_arr.shape, d_arr.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "* the device ndarray cannot be use as a numpy ndarray\n",
    "* it implements a few array methods/attributes compatible with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To copy a device array back to the host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr2 = d_arr.copy_to_host()   # copy to host\n",
    "print(arr2)\n",
    "assert np.all(arr == arr2)    # verify the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ufuncs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ufuncs provides a high-level API for creating elementwise array functions that can target CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import vectorize\n",
    "import math\n",
    "\n",
    "@vectorize(['float32(float32, float32)', \n",
    "            'float64(float64, float64)']) # default to cpu\n",
    "def cpu_ufunc_math(x, y):\n",
    "    return math.sin(x) / math.cos(y)\n",
    "\n",
    "\n",
    "@vectorize(['float32(float32, float32)', \n",
    "            'float64(float64, float64)'], target='cuda')\n",
    "def gpu_ufunc_math(x, y):\n",
    "    return math.sin(x) / math.cos(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = np.random.random(10**5).astype(np.float32)\n",
    "\n",
    "cpu_res = cpu_ufunc_math(arr, arr)\n",
    "gpu_res = gpu_ufunc_math(arr, arr)  # implicit memory transfer\n",
    "assert np.allclose(cpu_res, gpu_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('CPU Ufunc')\n",
    "%timeit cpu_ufunc_math(arr, arr)\n",
    "print('\\nGPU Ufunc with implicit transfer')\n",
    "%timeit gpu_ufunc_math(arr, arr)\n",
    "\n",
    "d_arr = cuda.to_device(arr)        # move data onto the device\n",
    "def call_gpu_ufunc_math(d_arr):\n",
    "    gpu_ufunc_math(d_arr, d_arr)   # using device memory directly; no memory transfer\n",
    "    cuda.synchronize()             # ensure completion; CUDA kernel is asynchronous\n",
    "\n",
    "print('\\nGPU Ufunc directly in device memory')\n",
    "%timeit call_gpu_ufunc_math(d_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing custom CUDA functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CUDA execution model defines two types of functions:\n",
    "\n",
    "**kernel**\n",
    "\n",
    "A kernel is callable from the host.  It is associated with a gridâ€”a group of thread-blocks.  Since a kernel launch is asynchronous to the host, it has no return value.  The computation result must be stored into output arguments.\n",
    "\n",
    "**device function**\n",
    "\n",
    "A device function is callable from the device (i.e. from kernels or device functions).  It behaves like normal functions and has a return value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thread Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A kernel launches with a grid.  A grid contains thread blocks and each thread block contains threads.  This forms a three tier thread hierarchy.  The kernel code is executed once per thread.  Blocks are scheduled concurrently as resources become available.  That means not all blocks are active at the same time, so inter-block dependencies are not possible.  All threads within a block are active when the block is active.  Therefore, intra-block thread communication is possible.\n",
    "\n",
    "A grid and a block can both be 1D, 2D, 3D. So there can be 6D to work with.  The coordinates of blocks and threads can be used to compute an identitier for the current thread.\n",
    "\n",
    "```python\n",
    "idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x \n",
    "idy = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
    "idz = cuda.threadIdx.z + cuda.blockIdx.z * cuda.blockDim.z\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a shorthand for this common task:\n",
    "\n",
    "```python\n",
    "idx = cuda.grid(1)\n",
    "idx, idy = cuda.grid(2)\n",
    "idx, idy, idz = cuda.grid(3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cuda grid](img/grid-of-thread-blocks.png)\n",
    "\n",
    "See: http://docs.nvidia.com/cuda/cuda-c-programming-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@cuda.jit                  # kernel function\n",
    "def copy_vector(data, output):\n",
    "    idx = cuda.grid(1)     # get global-id for the current thread for a 1D grid\n",
    "    if idx < output.size:  # if index is in-range\n",
    "        output[idx] = data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare input and output memory\n",
    "arr = np.arange(100)\n",
    "out = np.zeros_like(arr)\n",
    "\n",
    "# prepare grid/block configuration for the launch\n",
    "blocksize = 128\n",
    "gridsize = int(math.ceil(arr.size / blocksize))\n",
    "\n",
    "# launch kernel\n",
    "copy_vector[gridsize, blocksize](arr, out)\n",
    "\n",
    "print(out)\n",
    "# verify result\n",
    "assert np.all(arr == out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def copy_vector3d(data, output):\n",
    "    idx, idy, idz = cuda.grid(3)\n",
    "    if idx < output.shape[0] and idy < output.shape[1] and idz < output.shape[2]:\n",
    "        output[idx, idy, idz] = data[idx, idy, idz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = np.arange(100).reshape(4, 5, 5)\n",
    "out = np.zeros_like(arr)\n",
    "\n",
    "# block of 2 x 2 x 2 threads\n",
    "blocksize = 2, 2, 2\n",
    "# compute gridsize as ceil(ndata/blocksize)\n",
    "gridsize = tuple(np.ceil(np.asarray(arr.shape) / np.asarray(blocksize)).astype(np.int))\n",
    "\n",
    "copy_vector3d[gridsize, blocksize](arr, out)\n",
    "\n",
    "print(out)\n",
    "assert np.all(arr == out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the device memory (a.k.a global memory), there are other memory units.  Numba exposes the _local memory_ and _shared memory_.  The local memory is for per-thread storage with a fixed size at compile time.  The shared memory is a per-block storage with a fixed size at compile time.  It is important as a way for fast intra-block communication among threads.\n",
    "\n",
    "![cuda memory](img/memory-hierarchy.png)\n",
    "\n",
    "See: http://docs.nvidia.com/cuda/cuda-c-programming-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate intra-block communication, we implement a parallel reduction algorithm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numba import float32\n",
    "\n",
    "# Create a device function for the operation by the reduction.\n",
    "# This is separated as a device function for demonstration purpose-only\n",
    "@cuda.jit(device=True)\n",
    "def add(x, y):          \n",
    "    return x + y\n",
    "\n",
    "@cuda.jit               # the reduction kernel function\n",
    "def block_reduce(arr, out):\n",
    "    # Get thread identity\n",
    "    idx = cuda.grid(1)\n",
    "    tid = cuda.threadIdx.x\n",
    "    blkid = cuda.blockIdx.x\n",
    "    blksz = cuda.blockDim.x\n",
    "    \n",
    "    # Declare a shared memory.\n",
    "    # Note: A shared memory is statically allocated.  \n",
    "    #       The shape must be constant at compile time.\n",
    "    sm = cuda.shared.array(shape=64, dtype=float32)\n",
    "    # load data into SM cooperatively\n",
    "    # Explained:\n",
    "    #    * load data if index is in range\n",
    "    #    * otherwise, initialize to 0\n",
    "    sm[tid] = arr[idx] if idx < arr.size else 0\n",
    "\n",
    "    # Core reduction logic\n",
    "    step = blksz // 2      # defines the active threads; init to 1st half of the block\n",
    "\n",
    "    while step > 0:\n",
    "        # The second half is added to the first half\n",
    "        if tid < step:\n",
    "            # All threads within the first half of the block compute concurrently\n",
    "            sm[tid] = add(sm[tid], sm[tid + step])\n",
    "        # Reduce active threads by half\n",
    "        step //= 2\n",
    "        # Synchronize threads in the block.\n",
    "        # This ensures all threads has reached this point.\n",
    "        cuda.syncthreads()  \n",
    "\n",
    "    # Save result\n",
    "    if idx < out.size:\n",
    "        out[idx] = sm[tid]\n",
    "\n",
    "arr = np.ones(128, dtype=np.float32)\n",
    "out = np.zeros_like(arr)\n",
    "block_reduce[2, 64](arr, out)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: currently the output array is showing all intermediate results of the parallel reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Discard unneeded intermediate reductions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the above ``block_reduce`` kernel to store the per-block reduction result only.  The `out` array should be of length 2.  Element 0 store the sum of the first 64 elements.  Element 1 store the next 64 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check result with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert out.size == 2\n",
    "assert out[0] == arr[:64].sum()\n",
    "assert out[1] == arr[64:].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below cell to see the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answer = b'CmZyb20gbnVtYmEgaW1wb3J0IGZsb2F0MzIKCkBjdWRhLmppdChkZXZpY2U9VHJ1ZSkgICMgZGV2aWNlIGZ1bmN0aW9uCmRlZiBhZGQoeCwgeSk6CiAgICByZXR1cm4geCArIHkKCgpAY3VkYS5qaXQKZGVmIGJsb2NrX3JlZHVjZShhcnIsIG91dCk6CiAgICBpZHggPSBjdWRhLmdyaWQoMSkKICAgIHRpZCA9IGN1ZGEudGhyZWFkSWR4LngKICAgIGJsa2lkID0gY3VkYS5ibG9ja0lkeC54CiAgICBibGtzeiA9IGN1ZGEuYmxvY2tEaW0ueAogICAgCiAgICBzbSA9IGN1ZGEuc2hhcmVkLmFycmF5KHNoYXBlPTY0LCBkdHlwZT1mbG9hdDMyKQogICAgIyBsb2FkIGRhdGEgaW50byBTTSBjb29wZXJhdGl2ZWx5CiAgICBzbVt0aWRdID0gYXJyW2lkeF0gaWYgaWR4IDwgYXJyLnNpemUgZWxzZSAwCgogICAgIyByZWR1Y3Rpb24KICAgIHN0ZXAgPSBibGtzeiAvLyAyCgogICAgd2hpbGUgc3RlcCA+IDA6CiAgICAgICAgaWYgdGlkIDwgc3RlcDoKICAgICAgICAgICAgc21bdGlkXSA9IGFkZChzbVt0aWRdLCBzbVt0aWQgKyBzdGVwXSkKICAgICAgICBzdGVwIC8vPSAyCiAgICAgICAgIyBzeW5jaHJvbml6ZSB0aHJlYWRzIGluIHRoZSBibG9jawogICAgICAgIGN1ZGEuc3luY3RocmVhZHMoKSAgCgogICAgIyBTYXZlIHJlc3VsdAogICAgaWYgdGlkID09IDA6CiAgICAgICAgb3V0W2Jsa2lkXSA9IHNtW3RpZF0KCiAgICAKYXJyID0gbnAuYXJhbmdlKDEyOCwgZHR5cGU9bnAuZmxvYXQzMikKb3V0ID0gbnAuemVyb3MoMiwgZHR5cGU9bnAuZmxvYXQzMikKYmxvY2tfcmVkdWNlWzIsIDY0XShhcnIsIG91dCkKCnByaW50KG91dCkK'\n",
    "import base64\n",
    "print(base64.b64decode(answer).decode('ascii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "*Copyright Continuum 2012-2016 All Rights Reserved.*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:iqt]",
   "language": "python",
   "name": "conda-env-iqt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
