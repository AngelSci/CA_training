{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Learning Objectives:](#Learning-Objectives:)\n",
    "\t* [HDF5 Summary](#HDF5-Summary)\n",
    "\t\t* [Composition](#Composition)\n",
    "\t\t* [Warning](#Warning)\n",
    "\t\t* [Questions](#Questions)\n",
    "\t\t* [Exploring an HDF5 file found \"in the wild\"](#Exploring-an-HDF5-file-found-\"in-the-wild\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Work with data stored in fast, hierarchical scientific data formats:\n",
    "  * HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HDF5 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details at https://www.hdfgroup.org/HDF5/doc/H5.intro.html\n",
    "\n",
    "1. HDF5 files that are accessed via h5py store and return numpy arrays\n",
    "2. HDF5 files are composed groups and datasets\n",
    "3. Storing numerical-ish data is strongly recommended\n",
    "4. Groups can be accessed like both Python dicts and like Unix filesystem paths\n",
    "```python\n",
    "# Full path\n",
    "hdf5_file['/group1/subgroup2/subsubgroup1']\n",
    "# Equivalent to:\n",
    "g = hdf5_file['group1']\n",
    "g['subgroup2/subsubgroup1']\n",
    "# Or to nested lookup:\n",
    "hdf5_file['group1']['subgroup2']['subsubgroup1']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We won't be covering HDF (aka HDF4).\n",
    "  * HDF5 and HDF4 are two different things, even though they are by the same group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 files are composed of **groups** and **datasets**.\n",
    "A group contains any number of groups and datasets plus supporting metadata.\n",
    "A dataset is a multidimensional array of data elements plus supporting metadata.\n",
    "\n",
    "HDF5 files are organized like UNIX paths.\n",
    "Every HDF5 file has a group (the root) at \"/\".\n",
    "\n",
    "HDF5 groups are somewhat similar to Python dicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have problems if you try to use both pytables and h5py at the same time.\n",
    "This has been fixed in recent versions, but some people still use old stuff!!\n",
    "\n",
    "* http://stackoverflow.com/questions/28333470/use-both-h5py-and-pytables-in-the-same-python-process\n",
    "* https://github.com/h5py/h5py/issues/390\n",
    "  \n",
    "**ALWAYS** close the HDF5 file not matter what, after each small sequence of access.  \n",
    "\n",
    "Since merely opening a file doesn't require any reads or writes, it is safest to enclose each operation you wish to perform in a `with open(\"myfile.hdf5\"): ...` block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have h5py installed in your conda environment run\n",
    "```\n",
    "% conda install -y h5py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 1: Let's make a file!\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "filename = \"tmp/my_first_hdf5.hdf5\"\n",
    "\n",
    "# h5py.File can take a driver=\"driver\", libver=\"latest|earliest\", \n",
    "# and userblock=<size> arguments. In general, leave those options alone unless\n",
    "#  - you are using parallel HDF5 (aka MPI). Then set driver=\"mpio\"\n",
    "#  - you have to squeeze every bit of performance from the application, \n",
    "#    and don't care if no-one else can use it. Then set libver=\"latest\"\n",
    "#  - userblock is NOT chunking. userblock is some space at the beginning of the \n",
    "#    file that really isn't a part of the file.\n",
    "my_first_hdf5 = h5py.File(filename, mode='w')\n",
    "my_first_hdf5.close()\n",
    "\n",
    "# Hurray! We made our first (rather boring) hdf5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Put something in the file\n",
    "with h5py.File(filename, mode='w') as my_first_hdf5:\n",
    "    data = list(range(1000))\n",
    "    my_first_hdf5['dataset1'] = data\n",
    "    \n",
    "# This example easily put Python a list into an HDF5 dataset\n",
    "# We can (sort of) put arbitrary Python things into HDF5, but we shouldn't. \n",
    "# What should we store? Numerical-ish things.\n",
    "# What should we not store? Whatever we want.\n",
    "#\n",
    "# Whatever! I do what I want! \n",
    "#   - Eric Cartman (S6E3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3: Read the data\n",
    "with h5py.File(filename, mode='r') as my_first_hdf5:\n",
    "    data2 = my_first_hdf5['dataset1']\n",
    "\n",
    "print(data2)\n",
    "# Hmmm. Instead of getting the data, we instead got a \"closed HDF5 dataset\".\n",
    "# This is because h5py is lazily loading data instead of loading everything at once.\n",
    "#\n",
    "# This is really good!\n",
    "# What would happen if our dataset was 200GB? Could we load all of that into memory at once?\n",
    "# Probably not. (Unless you are very lucky to have access to a server with that much RAM)\n",
    "# But even if we have the memory, it probably doesn't make sense to load the whole thing \n",
    "# and then start processing it is probably smarter to iteratively load and process the \n",
    "# data in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3a: Actually read the data\n",
    "with h5py.File(filename, mode='r') as my_first_hdf5:\n",
    "    data2 = my_first_hdf5['dataset1'][:]\n",
    "\n",
    "print(type(data2))\n",
    "print(data2[:10])\n",
    "# We put a Python list into the dataset, but got a numpy array out.\n",
    "# Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Let's play with groups\n",
    "with h5py.File(filename, mode='w') as my_first_hdf5:\n",
    "    g1 = my_first_hdf5.create_group(\"first\")\n",
    "    # We can create nested groups automatically\n",
    "    # second, third, and fourth will each be different groups\n",
    "    g2 = my_first_hdf5.create_group(\"second/third/fourth\")\n",
    "    # We can create groups under a previously created group\n",
    "    # Note: g1.create_group instead of my_first_hdf5.create_group\n",
    "    g3 = g1.create_group(\"nestedfirst\")\n",
    "    g4 = g1.create_group(\"nestedsecond\")\n",
    "    #Now the group \"first\" has \n",
    "    \n",
    "    g5 = my_first_hdf5.create_group(\"first/nestedthird\")\n",
    "    \n",
    "# Questions:\n",
    "# Where is group \"first\"? group \"second\"?\n",
    "# How many groups are nested under \"first\"?\n",
    "# What is the absolute path to group \"nestedsecond\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What is a group?\n",
    "# What is a dataset?\n",
    "# Can a group contain another group?\n",
    "# Can a group contain a dataset?\n",
    "    \n",
    "with h5py.File(filename, mode='r') as my_first_hdf5:\n",
    "    list_of_groups = []\n",
    "    # visit() recursively visits every group and dataset in a file\n",
    "    # It calls the function that is given as an argument, stopping\n",
    "    #  if that function returns anything other than None\n",
    "    my_first_hdf5.visit(list_of_groups.append)\n",
    "    #my_first_hdf5.visit(print)\n",
    "\n",
    "list_of_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4a: Let's play with groups\n",
    "with h5py.File(filename, mode='w') as my_first_hdf5:\n",
    "    g1 = my_first_hdf5.create_group(\"first\")\n",
    "    # We can create nested groups automatically\n",
    "    # second, third, and fourth will each be different groups\n",
    "    g2 = my_first_hdf5.create_group(\"second/third/fourth\")\n",
    "    # We can create groups under a previously created group\n",
    "    # Note: g1.create_group instead of my_first_hdf5.create_group\n",
    "    g3 = g1.create_group(\"nestedfirst\")\n",
    "    g4 = g1.create_group(\"nestedsecond\")\n",
    "    # Now the group \"first\" has \n",
    "    \n",
    "    g5 = my_first_hdf5.create_group(\"first/nestedthird\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Where is group \"first\"? group \"second\"?\n",
    "2. How many groups are nested under \"first\"?\n",
    "3. What is the absolute path to group \"nestedsecond\"?\n",
    "4. What is a group?\n",
    "5. What is a dataset?\n",
    "6. Can a group contain another group?\n",
    "7. Can a group contain a dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 5: Combining groups and datasets\n",
    "filename = \"tmp/my_second_hdf5.hdf5\"\n",
    "data = [[i+j*10 for i in range(10)] for j in range(100)]\n",
    "data2 = np.arange(1000).reshape((10,20,5))\n",
    "\n",
    "with h5py.File(filename, mode='w') as f:\n",
    "    g = f.create_group(\"data\")\n",
    "    dset1 = g.create_dataset(\"dataset1\", (100,10), np.dtype('i8'), data=data)\n",
    "    # We could also have done it like so:\n",
    "    # f['data/dataset1'] = data\n",
    "    # What is the difference? create_dataset() is more flexible. It allows us to\n",
    "    #  - specify size and shape\n",
    "    #  - specify datatype\n",
    "    #  - specify chunking\n",
    "    #  - specify transparent compression\n",
    "    #  - specify resizability\n",
    "    dset2 = g.create_dataset(\"dataset2\", data2.shape)\n",
    "    dset2 = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with h5py.File(filename, mode='r') as f:\n",
    "    dset1 = f['data/dataset1'][:]\n",
    "    dset2 = f['data/dataset2'][:]\n",
    "    \n",
    "print(dset1.shape, \"\\n\", dset1[:1])\n",
    "print(dset2.shape, \"\\n\", dset2[:1])\n",
    "# Why is dset2 full of zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with h5py.File(filename, mode='w') as f:\n",
    "    g = f.create_group(\"data\")\n",
    "    # Option 1:\n",
    "    dset2 = g.create_dataset(\"dataset2\", shape=data2.shape, dtype=data2.dtype)\n",
    "    dset2[:] = data2\n",
    "    # The [:] is important!\n",
    "    \n",
    "    # Option 2:\n",
    "    # f['dataset2'] = data2\n",
    "\n",
    "with h5py.File(filename, mode='r') as f:\n",
    "    dset2 = f['data/dataset2'][:]\n",
    "    \n",
    "print(dset1.shape, \"\\n\", dset1[:1])\n",
    "print(dset2.shape, \"\\n\", dset2[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Iterating over datasets is also easy.\n",
    "# Remember, each dataset is basically a numpy array that is read from disk on demand\n",
    "with h5py.File(filename, mode='r') as f:\n",
    "    for item in f['data/dataset2']:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 6: Deleting datasets from a file\n",
    "filename = \"tmp/my_third_hdf5.hdf5\"\n",
    "\n",
    "with h5py.File(filename, \"w\") as f:\n",
    "    f['data/dataset1'] = np.arange(100000).reshape(10,10000)\n",
    "\n",
    "%ls -l $filename\n",
    "with h5py.File(filename, \"r+\") as f:\n",
    "    del f['data/dataset1']\n",
    "    %ls -l $filename\n",
    "\n",
    "#The dataset isn't actually deleted until the file is closed\n",
    "%ls -l $filename\n",
    "\n",
    "with h5py.File(filename, \"r+\") as f:\n",
    "    try:\n",
    "        del f['data/dataset1']\n",
    "    except KeyError:\n",
    "        print(\"Trying to delete dataset that doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 6a: Deleting entire groups\n",
    "with h5py.File(filename, \"w\") as f:\n",
    "    f['data/dataset1'] = np.arange(100000).reshape(10,10000)\n",
    "    f['data/dataset2'] = np.arange(100000,200000).reshape(10,10000)\n",
    "    f['data/dataset3'] = np.arange(200000,300000).reshape(10,10000)\n",
    "    \n",
    "%ls -l $filename\n",
    "with h5py.File(filename, \"r+\") as f:\n",
    "    del f['data']\n",
    "    %ls -l $filename\n",
    "\n",
    "# The dataset isn't actually deleted until the file is closed\n",
    "%ls -l $filename\n",
    "\n",
    "with h5py.File(filename, \"r+\") as f:\n",
    "    l = []\n",
    "    f.visit(l.append)\n",
    "\n",
    "# Notice that the file didn't shrink to a small number of bytes.\n",
    "# The datasets and group have been unlinked, but the space hasn't been reclaimed.\n",
    "# To shrink the file, we need to run an \"h5repack\" on it.\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 7: Updating an existing dataset\n",
    "filename = \"tmp/my_fourth_hdf5.hdf5\"\n",
    "\n",
    "with h5py.File(filename, \"w\") as f:\n",
    "    f['data/dataset1'] = np.arange(100000).reshape(10000,10)\n",
    "    f['data/dataset2'] = np.arange(100000,200000).reshape(10000,10)\n",
    "    f['data/dataset3'] = np.arange(200000,300000).reshape(10000,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 7: Updating datasets\n",
    "with h5py.File(filename, \"r+\") as f:\n",
    "    print(f['data/dataset1'][:10])\n",
    "    f['data/dataset1'][:5] = -1\n",
    "    \n",
    "with h5py.File(filename, \"r+\") as f:\n",
    "    print(f['data/dataset1'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 8: resizing existing datasets\n",
    "d1 = np.arange(100000).reshape(10000,10)\n",
    "with h5py.File(filename, \"w\") as f:\n",
    "    # make a new dataset that can grow to 10x the initial size\n",
    "    dset1 = f.create_dataset(\"resizable/dataset1\", d1.shape, \n",
    "                             maxshape=(d1.shape[0]*10, d1.shape[1]))\n",
    "    dset1[:] = d1\n",
    "    \n",
    "    # Here is an alternate way to create the dataset\n",
    "    # f.create_dataset(\"resizable/dataset1\", d1.shape, \n",
    "    #                  maxshape=(d1.shape[0]*10, d1.shape[1]), data=d1)\n",
    "%ls -l $filename    \n",
    "\n",
    "with h5py.File(filename, \"r+\") as f:\n",
    "    # double the size of the dataset\n",
    "    dset1 = f[\"resizable/dataset1\"]\n",
    "    print(dset1.shape)\n",
    "    print(dset1.maxshape)\n",
    "    dset1.resize(dset1.shape[0]*2, axis=0)\n",
    "    print(dset1.shape)\n",
    "    \n",
    "    dset1[dset1.shape[0]//2:] = d1\n",
    "\n",
    "%ls -l $filename\n",
    "with h5py.File(filename, \"r+\") as f:\n",
    "    # Check that the dataset is actually the size we want\n",
    "    dset1 = f[\"resizable/dataset1\"]\n",
    "    d1 = dset1[:]\n",
    "    print(d1.shape)\n",
    "    print(d1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "raises": "ValueError"
   },
   "outputs": [],
   "source": [
    "with h5py.File(filename, \"r+\") as f:\n",
    "    # resize again, past our original limit\n",
    "    dset1 = f[\"resizable/dataset1\"]\n",
    "    print(dset1.shape)\n",
    "    print(dset1.maxshape)\n",
    "    dset1.resize(dset1.shape[0]*6, axis=0)\n",
    "    print(dset1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for datasets to be resized, they *must* be chunked.\n",
    "\n",
    "This chunking happens automatically in some cases, but can be specified. Chunking happens automatically when:\n",
    "\n",
    "- compression is turned on\n",
    "- maxshape is specified for the dataset\n",
    "\n",
    "Intuition about chunking\n",
    "\n",
    "- Specifying the chunk size is easy to get wrong! Especially when multiple subtle factors are in play:\n",
    "  - Chunk size\n",
    "  - Compression\n",
    "  - Chunk cache size\n",
    "  - Underlying disk subsystem (especially for parallel filesystems)\n",
    "\n",
    "http://www.hdfgroup.org/HDF5/doc/Advanced/Chunking/\n",
    "http://www.hdfgroup.org/HDF5/doc/Advanced/Chunking/Chunking_Tutorial_EOS13_2009.pdf\n",
    "\n",
    "**If the chunk size is wrong, accessing the data can be 10-100 times slower than normal.**\n",
    "\n",
    "Moral of the story: Don't set chunking yourself unless you can conclusively demonstrate that it is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step 9: HDF5 Attributes on Groups and Datasets\n",
    "#Step 10: Transparent compression\n",
    "# - Why transparent compression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring an HDF5 file found \"in the wild\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "metadata = \"data/Granule_Metadata.xml\"\n",
    "collection = \"data/GES_DISC_GPM_3GPROFF16SSMIS_DAY_V03_dif.xml\"\n",
    "hdf5_precip = \"data/3A-DAY.F16.SSMIS.GRID2014R2.20150101-S000000-E235959.001.V03C.HDF5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import webbrowser, os\n",
    "try:\n",
    "    from urllib.parse import quote\n",
    "except ImportError:\n",
    "    from urllib import quote # Python 2.7\n",
    "webbrowser.open(\"file:///%s/%s\" % (os.getcwd(), quote(metadata)))\n",
    "webbrowser.open(\"file:///%s/%s\" % (os.getcwd(), quote(collection)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = h5py.File(hdf5_precip, \"r\")\n",
    "list(f.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f['InputFileNames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f['InputFileNames'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputFileNames = list(f['InputFileNames'])[0].decode().split(',')\n",
    "inputFileNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_datasets = list(f['Grid'])\n",
    "grid_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rain = f['Grid']['liquidPrecipFraction']\n",
    "print(rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rain[:5,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice a lot of these apparently sentinal values in the datasets. The value -9999.90039062 seems to be used as a filled-in number in a presumably sparse array (the file size isn't large enough to hold all the data if it was non-sparse, as we will see)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let us see which datasets have meaningful values, and how commonly\n",
    "from operator import mul\n",
    "from functools import reduce\n",
    "\n",
    "for dataset in grid_datasets:\n",
    "    data = f['Grid'][dataset]\n",
    "    non_sentinal = data[:] >= -9999\n",
    "    print(dataset, \"has real data in %d of %d positions\" % (\n",
    "                    non_sentinal.sum(), reduce(mul, data.shape, 1)))\n",
    "    print(\"-\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(rain[:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(rain[705:716,400:411])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drizzle = (.1 < rain[:]) & (rain[:] < .9)              \n",
    "drizzle.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "times = f['InputGenerationDateTimes']\n",
    "times[0].decode('utf-8').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(f.attrs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f.attrs['FileInfo'].decode('utf-8').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f.attrs['FileHeader'].decode('utf-8').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We've already seen that mixedWater is only those sentinal values\n",
    "# But just want to show how to use a Pandas Panel for N dimensions\n",
    "mixedWater = f['Grid']['mixedWater']\n",
    "panel = pd.Panel(f['Grid']['mixedWater'][:])\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "panel[10:15,700,400:411]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic creation of a new HDF5 data file is done with:\n",
    "\n",
    "```python\n",
    ">>> import h5py\n",
    ">>> import numpy as np\n",
    ">>> f = h5py.File(\"mytestfile.hdf5\", \"w\")\n",
    ">>> dset = f.create_dataset(\"mydataset\", (100,), dtype='i')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "continuum": {
   "depends": [
    "ip_containers",
    "ip_files"
   ],
   "requires": [
    "data/Granule_Metadata.xml",
    "data/GES_DISC_GPM_3GPROFF16SSMIS_DAY_V03_dif.xml",
    "data/3A-DAY.F16.SSMIS.GRID2014R2.20150101-S000000-E235959.001.V03C.HDF5",
    "data/dataset1",
    "data/dataset2",
    "data/dataset3"
   ],
   "tag": "data_hdf5"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
