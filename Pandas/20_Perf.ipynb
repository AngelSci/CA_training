{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Learning Objectives](#Learning-Objectives)\n",
    "* [Pandas and Performance](#Pandas-and-Performance)\n",
    "\t* [Set-Up](#Set-Up)\n",
    "* [Numba & Cython](#Numba-&-Cython)\n",
    "* [Dask](#Dask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this notebook, the learner will be able to:\n",
    "* Use Numba and Cython to improve computational speed of operations on panadas containers\n",
    "* Use Dask to perform simple parallel processing tasks on pandas containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pandas and Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are focusing on the storage/execution part of the PyData Stack. Pandas will often defer certain computations to various engines. \n",
    "\n",
    "The most familiar is ``numpy`` (which serves as the storage back-end as well). \n",
    "\n",
    "For reductions, we use ``bottleneck``, which is quite efficient at things like ``nansum`` (summing across a 1-d array with nans).\n",
    "\n",
    "When using ``.query()`` or ``.eval()``, we defer the computation to ``numexpr``, which can evaluate from a string, multiple computations in a single expression. Furthermore it can operation in using multiple cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![PyData ecosystem](img/pydata-ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 8\n",
    "pd.options.display.max_columns = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numba & Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pandas.pydata.org/pandas-docs/stable/enhancingperf.html\n",
    "\n",
    "We are going to use a ``numba`` JIT (just-in-time) compiler, and ``cython`` (a static compiler) that allow one to write python and have it run at c-like speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import cython\n",
    "%load_ext cython\n",
    "\n",
    "np.random.seed(1234)\n",
    "pd.set_option('max_row',12)\n",
    "s = pd.Series(np.random.randn(1e5))\n",
    "com = 20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to compare 4 different expressions of the same computation for exponential weighted moving average (EWMA). This is a calculation that cannot be easily vectorized, as its a recurrence relation, that is, you have to do a computation for a particular row, then proceed to the next one.\n",
    "\n",
    "- python\n",
    "- cython1\n",
    "- cython2 (the actual pandas impl)\n",
    "- numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python implementation is straightforward and serves as a refernce impl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def python(s):\n",
    "    output = pd.Series(index=range(len(s)))\n",
    "\n",
    "    alpha = 1. / (1. + com)\n",
    "    old_weight = 1.0\n",
    "    new_weight = 1.0\n",
    "    weighted_avg = s[0]\n",
    "    output[0] = weighted_avg\n",
    "    \n",
    "    for i in range(1,len(s)):\n",
    "        v = s[i]\n",
    "        old_weight *= (1-alpha)\n",
    "        weighted_avg = ((old_weight * weighted_avg) + \n",
    "                        (new_weight * v)) / (old_weight + new_weight)\n",
    "        old_weight += new_weight\n",
    "        output[i] = weighted_avg\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the python implemtation and type the variables in cython. We expose this cython function via a python function wrapper that creates and returns a Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "skip_test": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "cimport cython\n",
    "@cython.wraparound(False)\n",
    "@cython.boundscheck(False)\n",
    "def _cython(double[:] arr, double com, double[:] output):\n",
    "    cdef:\n",
    "        double alpha, old_weight, new_weight, weighted_avg, v\n",
    "        int i\n",
    "    \n",
    "    alpha = 1. / (1. + com)\n",
    "    old_weight = 1.0\n",
    "    new_weight = 1.0\n",
    "    weighted_avg = arr[0]\n",
    "    output[0] = weighted_avg\n",
    "    \n",
    "    for i in range(1,arr.shape[0]):\n",
    "        v = arr[i]\n",
    "        old_weight *= (1-alpha)\n",
    "        weighted_avg = ((old_weight * weighted_avg) + \n",
    "                        (new_weight * v)) / (old_weight + new_weight)\n",
    "        old_weight += new_weight\n",
    "        output[i] = weighted_avg\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "skip_test": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def cython1(s):\n",
    "    output = np.empty(len(s),dtype='float64')\n",
    "    _cython(s.values, com, output)\n",
    "    return Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the pandas implementation of ewma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "skip_test": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def cython2(s):\n",
    "    return s.ewm(com=com,adjust=True).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the numba implementation, looks very similar to the python impl, just with the ``@jit`` decorator. We are also wrapping this in a function to get/return a Series. In newer numba implementations, we can also do the array allocation INSIDE the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "skip_test": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def _numba(arr, output):\n",
    "    alpha = 1. / (1. + com)\n",
    "    old_weight = 1.0\n",
    "    new_weight = 1.0\n",
    "    weighted_avg = arr[0]\n",
    "    output[0] = weighted_avg\n",
    "    \n",
    "    for i in range(1,arr.shape[0]):\n",
    "        v = arr[i]\n",
    "        old_weight *= (1-alpha)\n",
    "        weighted_avg = ((old_weight * weighted_avg) + \n",
    "                        (new_weight * v)) / (old_weight + new_weight)\n",
    "        old_weight += new_weight\n",
    "        output[i] = weighted_avg\n",
    "    \n",
    "\n",
    "def numba(s):\n",
    " \n",
    "    output = np.empty(len(s),dtype='float64')\n",
    "    _numba(s.values, output)\n",
    "    return Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important thing in comparing performance is correctness!! We need to be sure that we are comparing the SAME things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "skip_test": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "result1 = python(s)\n",
    "result2 = cython1(s)\n",
    "result3 = cython2(s)\n",
    "result4 = numba(s)\n",
    "result1.equals(\n",
    "    result2) and result1.equals(\n",
    "    result3) and result1.equals(\n",
    "    result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "skip_test": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%timeit -n 1 -r 1 python(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "skip_test": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit cython1(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "skip_test": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit cython2(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "skip_test": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit numba(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be suprising that the pandas implmentation takes a bit longer than cython/numba. We are doing additional work inside this impl though; meaning, we are doing ``NaN`` checking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is a library to help you with out-of-core calculations and parallelization. In this case we are using all of cores to do this computation. pandas has recently released the global-interpreter-lock (GIL) in order to faciliation this type of multi-threaded/core computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dask.readthedocs.org/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask import threaded, multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "N = int(1e7)\n",
    "df = pd.DataFrame({'key' : np.random.randint(0,1000,size=N), \n",
    "                'value' : np.random.randn(N)})\n",
    "ddf = dd.from_pandas(df, npartitions=8)\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code looks remarkably like the pandas code above, with the exception of the ``.compute()`` method call. This is by design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%timeit df.groupby('key').value.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit ddf.groupby('key').value.sum().compute(get=threaded.get)"
   ]
  }
 ],
 "metadata": {
  "continuum": {
   "depends": [
    "pd_intro"
   ],
   "requires": [
    "data/games.hdf"
   ],
   "tag": "pd_perf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
