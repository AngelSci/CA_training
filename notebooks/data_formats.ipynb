{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/logo.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/title.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/py3k.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Learning Objectives:](#Learning-Objectives:)\n",
    "\t* [Things to know about CSV](#Things-to-know-about-CSV)\n",
    "\t\t* [CSV files are not well-structured](#CSV-files-are-not-well-structured)\n",
    "\t\t* [When to use CSV](#When-to-use-CSV)\n",
    "\t\t* [Basic Steps for Dealing with CSV](#Basic-Steps-for-Dealing-with-CSV)\n",
    "\t\t* [File structure](#File-structure)\n",
    "* [The `csv` module](#The-csv-module)\n",
    "\t* [Manually converting the data fields](#Manually-converting-the-data-fields)\n",
    "\t* [Detecting the format](#Detecting-the-format)\n",
    "* [Just use `pandas`](#Just-use-pandas)\n",
    "* [Not-quite-CSV: Eyeballing the data](#Not-quite-CSV:-Eyeballing-the-data)\n",
    "* [Working with Spreadsheets](#Working-with-Spreadsheets)\n",
    "\t* [What are Spreadsheets?](#What-are-Spreadsheets?)\n",
    "\t\t* [What are spreadsheets good for?](#What-are-spreadsheets-good-for?)\n",
    "\t* [Structure of Excel files](#Structure-of-Excel-files)\n",
    "\t\t* [New XML-based hotness: .xlsx ](#New-XML-based-hotness:-.xlsx)\n",
    "\t\t* [Old binary-format-based, but not busted: .xls](#Old-binary-format-based,-but-not-busted:-.xls)\n",
    "\t* [Structure of ODT (and ODS) files](#Structure-of-ODT-%28and-ODS%29-files)\n",
    "\t\t* [XML-based .odt and .ods](#XML-based-.odt-and-.ods)\n",
    "\t\t* [Picking one to use:](#Picking-one-to-use:)\n",
    "\t* [Basic Steps for Programmatically Working with Excel](#Basic-Steps-for-Programmatically-Working-with-Excel)\n",
    "\t* [Notes and Gotchas](#Notes-and-Gotchas)\n",
    "\t* [Exercises](#Exercises)\n",
    "\t* [Optional Exercises](#Optional-Exercises)\n",
    "\t\t* [What is a cell?](#What-is-a-cell?)\n",
    "* [Machine and Human Readable Formats](#Machine-and-Human-Readable-Formats)\n",
    "\t* [Scale of difficulty](#Scale-of-difficulty)\n",
    "\t* [Common uses](#Common-uses)\n",
    "\t* [Terms](#Terms)\n",
    "\t* [JSON : JavaScript Object Notation   ](#JSON-:-JavaScript-Object-Notation)\n",
    "\t\t* [Why JSON?](#Why-JSON?)\n",
    "\t\t* [Why not JSON?](#Why-not-JSON?)\n",
    "\t* [YAML: YAML Ain't Markup Language](#YAML:-YAML-Ain't-Markup-Language)\n",
    "\t\t* [Why YAML?](#Why-YAML?)\n",
    "\t\t* [Why not YAML?](#Why-not-YAML?)\n",
    "\t* [XML: eXtensible Markup Language](#XML:-eXtensible-Markup-Language)\n",
    "\t\t* [Why (should you use) XML?](#Why-%28should-you-use%29-XML?)\n",
    "\t\t* [Why (should you) not (use) XML?](#Why-%28should-you%29-not-%28use%29-XML?)\n",
    "\t* [JSON](#JSON)\n",
    "\t* [YAML](#YAML)\n",
    "\t* [XML](#XML)\n",
    "\t\t* [expat](#expat)\n",
    "\t\t* [ElementTree](#ElementTree)\n",
    "\t\t* [SAX (Simple API for XML)](#SAX-%28Simple-API-for-XML%29)\n",
    "\t\t* [DOM (Document Object Model)](#DOM-%28Document-Object-Model%29)\n",
    "\t* [Exercise (representing and processing XML)](#Exercise-%28representing-and-processing-XML%29)\n",
    "\t* [HDF5 Summary](#HDF5-Summary)\n",
    "\t\t* [Composition](#Composition)\n",
    "\t\t* [Warning](#Warning)\n",
    "\t\t* [Questions](#Questions)\n",
    "\t\t* [Exploring an HDF5 file found \"in the wild\"](#Exploring-an-HDF5-file-found-\"in-the-wild\")\n",
    "\t* [NetCDF](#NetCDF)\n",
    "\t* [Exercise (export to scientific formats)](#Exercise-%28export-to-scientific-formats%29)\n",
    "\t* [Review of HDF5 and NetCDF](#Review-of-HDF5-and-NetCDF)\n",
    "* [IDL .sav files](#IDL-.sav-files)\n",
    "* [Learning Objectives](#Learning-Objectives)\n",
    "\t* [Preamble](#Preamble)\n",
    "* [Sqlite3](#Sqlite3)\n",
    "* [PostgreSQL (and DBAPI generally)](#PostgreSQL-%28and-DBAPI-generally%29)\n",
    "\t* [Fortran 77 Unformatted](#Fortran-77-Unformatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completion of this module, learners should be able to:\n",
    "\n",
    "* Read from and write to delimited data files, such as CSV\n",
    "* Learn how to do so robustly\n",
    "* Learn why to not do so if possible\n",
    "* Understand the structure of Excel .xlsx files\n",
    "* Read data from Excel files\n",
    "* Write data to Excel files\n",
    "* Learn what JSON, YAML, and XML are\n",
    "* Learn when and why to use them\n",
    "* Learn how to manipulate and construct each type\n",
    "* Learn the limitations and risks associated with each\n",
    "* Work with formats that mirror the native data structures of Python:\n",
    "* JSON\n",
    "* YAML\n",
    "* Work with XML data using several APIs:\n",
    "* expat\n",
    "* ElementTree\n",
    "* SAX (Simple API for XML)\n",
    "* DOM (Document Object Model\n",
    "* Work with data stored in fast, hierarchical scientific data formats:\n",
    "* HDF5\n",
    "* NetCDF\n",
    "* IDL .sav files\n",
    "* Fortran 77 Unformatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to know about CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV files are not well-structured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* They don't include data types\n",
    "* They don't enforce structure\n",
    "* They are not standard\n",
    "  * CSV == \"Comma-Separated Values\"\n",
    "  * Is it actually comma-separated? No. (See [Wikipedia](https://en.wikipedia.org/wiki/Comma-separated_values) or [RFC 4180](https://tools.ietf.org/html/rfc4180))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only reason to use CSV is when backwards-compatibility is required and and you can't fix the original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Steps for Dealing with CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Look at a small sample of the input\n",
    "  1. Check delimiters\n",
    "  2. Check types\n",
    "2. Read in a small sample of the input and convert it to the necessary format(s)\n",
    "3. Robust-ify the code with error catching and exception handling\n",
    "4. Test on a large sample of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now that we know everything about CSV, let's examine our data\n",
    "import csv\n",
    "aapl_stocks = \"data/AAPL.csv\"\n",
    "aapl_stocks_01 = \"data/AAPL01.csv\"\n",
    "#The \"head\" command will show the first ten lines of the file.\n",
    "#It gives it to us in raw form.\n",
    "!wc $aapl_stocks_01\n",
    "print()\n",
    "!head -6 $aapl_stocks_01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row is a header that tells us the name of the columns.\n",
    "* Is that header required?\n",
    "* Is it possible to have multiple header rows?\n",
    "* How many fields are there?\n",
    "* What is the field delimeter?\n",
    "\n",
    "What is the format of each line after that?\n",
    "* Date, floating point, floating point, floating point, floating point, integer, floating point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `csv` module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the easy case, a very simple use of the `csv` module gives us useful results.  Perhaps not perfect, but a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic comma-separated file\n",
    "import csv\n",
    "with open(aapl_stocks) as csvfile:\n",
    "    stockreader = csv.reader(csvfile)\n",
    "    for n, row in zip(range(6), stockreader):\n",
    "        print(\"%d:\" % n, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might have made some wrong assumptions in the dialect of CSV being used though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Of course, we assumed a comma is the actual delimiter\n",
    "with open(aapl_stocks_01) as csvfile:\n",
    "    stockreader = csv.reader(csvfile)\n",
    "    for n, row in zip(range(6), stockreader):\n",
    "        print(\"%d:\" % n, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It looks like this actually uses tabs; happily the module can deal with many variations\n",
    "with open(aapl_stocks_01) as csvfile:\n",
    "    stockreader = csv.reader(csvfile, delimiter='\\t')\n",
    "    for n, row in zip(range(6), stockreader):\n",
    "        print(\"%d:\" % n, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually converting the data fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from datetime import datetime\n",
    "with open(aapl_stocks, 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    #Consume the file header, and then convert each line's \n",
    "    # data so that it has the type we need\n",
    "    lines = [line for line in csvreader]\n",
    "    header = lines.pop(0)\n",
    "    data = [ [datetime.strptime(line[0],'%Y-%m-%d'), float(line[1]), \n",
    "              float(line[2]), float(line[3]), \n",
    "              float(line[4]), int(line[5]), float(line[6])] \n",
    "            for line in lines]\n",
    "for line in data:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We can determine useful information about input using the CSV sniffer\n",
    "with open(aapl_stocks, 'r') as csvfile:\n",
    "    sample = csvfile.read(4096)\n",
    "    sniffer = csv.Sniffer()\n",
    "    dialect = sniffer.sniff(sample)\n",
    "    has_header = sniffer.has_header(sample)\n",
    "\n",
    "print(\"Has header:\".rjust(20), has_header)\n",
    "print(\"Delimiter:\".rjust(20), repr(dialect.delimiter))\n",
    "print(\"Double quote:\".rjust(20), dialect.doublequote)\n",
    "print(\"Escape character:\".rjust(20), dialect.escapechar)\n",
    "print(\"Line terminator:\".rjust(20), repr(dialect.lineterminator))\n",
    "print(\"Quote character:\".rjust(20), dialect.quotechar)\n",
    "print(\"Quoting:\".rjust(20), dialect.quoting)\n",
    "print(\"Skip initial space:\".rjust(20), dialect.skipinitialspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just use `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The truth is that a lot of these issues have been handled well by the Pandas library.  It's `.read_csv()` function comes with dozens of named arguments for dealing with the many edge cases in how real-world files are formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "aapl = pd.read_csv('data/AAPL.csv', index_col='Date')\n",
    "aapl[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not-quite-CSV: Eyeballing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# More convoluted tab-separated with header lines, etc.\n",
    "# Let's try to figure out how to work with the data\n",
    "cowlitz_file = 'data/cowlitz_river_wa_usgs_flow_data.rdb'\n",
    "cowlitz = open(cowlitz_file).readlines()\n",
    "for line in cowlitz[:32]:\n",
    "    print(line.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subject area experts will find the format familiar, I am sure.  The *rdb* format is described at http://help.waterdata.usgs.gov/faq/about-tab-delimited-output as well.  But I am a non-expert in the subject area, so I will just visually examine it, and figure out in a relatively ad hoc way how to read and utilize it.\n",
    "\n",
    "Here are some things I notice:\n",
    "\n",
    "* The file starts with a commented header, with each line beginning with a hash mark (`# `) and space.\n",
    "* The next line after the header is a list of field names.\n",
    " * Some field names start with numbers, and are not valid Python identifiers.\n",
    "* The next line after the field names is the data types of the columns; but I'm not sure exactly what those descriptions mean.\n",
    "* The bulk of the file is tab-separated values.\n",
    "\n",
    "Let's write a small custom function to parse what we see in this data format. Note that I actually *did* a quick search, and it appears the modules `Asciitable` and the package `Astropy` both seem to support this format (other existing libraries might also); but suppose it was something novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_rdb(filename):\n",
    "    from collections import namedtuple, OrderedDict\n",
    "    fh = open(filename)\n",
    "    # First collect the comments, stopping at the field names\n",
    "    comment_lines = []\n",
    "    for line in fh:\n",
    "        # We've gotten to the header\n",
    "        if not line.startswith('#'):\n",
    "            fields = line.rstrip().split('\\t')\n",
    "            break\n",
    "        comment_lines.append(line[2:])\n",
    "    # Make the individual lines into one string\n",
    "    comment = ''.join(comment_lines)\n",
    "    # Read the next line with the data formats\n",
    "    formats = next(fh).rstrip().split('\\t')\n",
    "    # Make sure field names are valid Python identifiers\n",
    "    field_names = [f if f[0].isalpha() else 'N_'+f for f in fields]\n",
    "    # Define header as ordered mapping of field name to data type\n",
    "    header = OrderedDict(zip(field_names, formats))\n",
    "    row = namedtuple('Row', field_names)\n",
    "    records = []\n",
    "    for values in csv.reader(fh, delimiter='\\t'):\n",
    "        records.append(row(*values))\n",
    "    # Close the file before we leave\n",
    "    fh.close()\n",
    "    return comment, header, records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment, header, cowlitz_data = read_rdb(cowlitz_file)\n",
    "for field, datatype in header.items():\n",
    "    print(\"%s: %s\" % (field, datatype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"%d records, show first five\" % len(cowlitz_data))\n",
    "\n",
    "print('----------')\n",
    "for record in cowlitz_data[:5]:\n",
    "    print(record)\n",
    "    \n",
    "print('----------')\n",
    "print(\"Work with a particular record in a straightforward way\")\n",
    "my_row = cowlitz_data[1000]\n",
    "print(my_row.datetime, my_row.site_no, my_row.N_01_00060_00003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len([r for r in cowlitz_data if r.N_01_00060_00003_cd=='A:e'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cowlitz_data, columns=header.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Spreadsheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Spreadsheets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spreadsheets are files that can only be modified via lots of mouse-clicking. (Or is that true?)\n",
    "* Databases\n",
    "* Todo Lists\n",
    "* Complex Programs\n",
    "* A catchall for data for people who don't/can't know any better. (This is not true, but it often feels true.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are spreadsheets good for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rapid prototyping\n",
    "* Easy to share understanding between technical and non-technical people\n",
    "* Concrete structure makes it easy for non-programmers (and it makes it dangerous)\n",
    "\n",
    "\n",
    "Microsoft Excel is the dominant spreadsheet program, so we'll focus on that, but give some examples with the ODT (Open DocumenT) championed by the Free Software community (specifically OASIS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of Excel files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New XML-based hotness: .xlsx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* xlsx defines the structure of Excel spreadsheets that fit into the [OOXML framework](http://www.officeopenxml.com/anatomyofOOXML-xlsx.php). \n",
    "* One .xlsx file contains only one workbook (but worksheets in that workbook may refer to other workbooks in other files).\n",
    "* A .xlsx file is actually a zip file (aka package) containing a number of parts. Some are required, some are not.\n",
    "  * [Content_Types].xml is required\n",
    "  * relationships between different things are required (between worksheets, styles, external resources, etc.)\n",
    "* A workbook may contain one or more worksheets\n",
    "* Each worksheet is kept in a different XML file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old binary-format-based, but not busted: .xls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* xls is a binary-format specification that defines the structure of Excel spreadsheets.\n",
    "* An xls file is \"... an OLE compound file. A compound file contains storages, streams, and substreams. Each stream or substream contains a series of binary records. Each binary record contains zero or more structured fields that contain the workbook data. (This brief excerpt taken from [MSDN](https://msdn.microsoft.com/en-us/library/office/cc313154%28v=office.12%29.aspx)\n",
    "* The basic building block of xls files is the binary record. Each record is a variable-length sequence of bytes, and is composed of three things: record type, record size, and data.\n",
    "\n",
    "In other words, xls is a complex format. (I hate this format now. But in truth, it is actually pretty amazing. Backwards compatible to the beginning of time, made to be fast on old computers (like the kind from 10+ years ago), and designed to solve the problems of the day while still being able to handle the future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of ODT (and ODS) files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML-based .odt and .ods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* odt defines the structure of ODS spreadsheets that fit into the [ISO/IEC 26300-1:2015 specification](http://www.iso.org/iso/home/store/catalogue_ics/catalogue_detail_ics.htm?csnumber=66363) \n",
    "* odt files are composed of many XML elements (spreadsheets, charts, images, text, drawings, etc.)\n",
    "* ods files are simply odt files that use the \"ods\" extension to tell what program should open the file.\n",
    "  * In other words, there is absolutely NOTHING special about .ods\n",
    "* Each spreadsheet element can contain table elements, calculation elements, and lots of other XML elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking one to use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Should you use a spreadsheet in the first place?\n",
    "  1. How is the data intended to be used?\n",
    "  2. How much data is there?\n",
    "  3. Who knows what logic and calculations have to be encoded? A business analyst or accountant?\n",
    "  4. \n",
    "2. If so...\n",
    "  3. Is backwards compatibility to Excel 2005 required? (Excel 2007 was one of the first versions to actually support OOXML, according to my \"Google archaeology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Steps for Programmatically Working with Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Look at a small sample of your data\n",
    "2. Test on a small sample of the data\n",
    "3. Robustify the code\n",
    "4. Test on a larger sample of the data\n",
    "5. Iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and Gotchas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python indexing is 0-based\n",
    "* Excel indexing is 1-based\n",
    "* This makes for a WEIRD mismash of indexing techniques\n",
    "  * worksheet.cell(row=1, column=1) == worksheet.rows[0][0]\n",
    "* openpyxl requires a LOT of memory, even for smallish spreadsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conda install openpyxl xlrd xlwt\n",
    "#This won't work: \"conda install xlutils\". It is apparently incompatible with python 3.4 (as of 2015-06-25)\n",
    "import openpyxl\n",
    "import xlrd\n",
    "import xlwt\n",
    "\n",
    "from openpyxl import load_workbook, Workbook\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "aapl_xlsx = \"data/AAPL01.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wb = load_workbook(aapl_xlsx)\n",
    "#A workbook should have one or more worksheets.\n",
    "#Let's see\n",
    "pprint(wb.worksheets)\n",
    "AAPL_ws = wb['AAPL']\n",
    "pprint(AAPL_ws)\n",
    "\n",
    "#for row in AAPL_ws.rows[1:10]:\n",
    "#    for cell in row[:7]:\n",
    "#        print (cell.value)\n",
    "\n",
    "#What is the difference bewteen that loop and this one?\n",
    "for row in AAPL_ws['A2':'F11']:\n",
    "    for cell in row:\n",
    "        print (cell.value)\n",
    "\n",
    "#The top loop is loading ALL columns (from A-ZZZZZZ whatever)\n",
    "#This is fine if you can wait a while and have lots o' RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Iterate over the opening prices and find and print the maximum\n",
    "\n",
    "#Why use \"maximum\" instead of easier to write \"max\"?\n",
    "maximum = float(\"-inf\")\n",
    "for cell in AAPL_ws.columns[1][1:]:\n",
    "    if maximum < float(cell.value):\n",
    "        maximum = float(cell.value)\n",
    "print(\"The highest opening price is {}\".format(maximum))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find and print the maximum volume\n",
    "2. Sum and print the volume over all time\n",
    "3. Find and print any differences between the closing price and adjusted closing prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find and print the maximum volume per year\n",
    "2. Find and print the maximum and minimum opening price per year\n",
    "3. Sum and print the volume over each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_first_workbook = \"data/my_first_spreadsheet.xlsx\"\n",
    "new_wb = Workbook()\n",
    "\n",
    "#Each workbook has at least one worksheet\n",
    "ws = new_wb.active\n",
    "ws.title = \"Test1\"\n",
    "\n",
    "\n",
    "ws.cell('A1').value = \"Header1\"\n",
    "ws.cell('B1').value = \"Header2\"\n",
    "ws.cell('C1').value = \"Header3\"\n",
    "ws.cell('D1').value = \"Header4\"\n",
    "\n",
    "for col in range(1,5):\n",
    "    for row in range(2,10):\n",
    "        c = ws.cell(column=col, row=row)\n",
    "        c.value = col*100 + row\n",
    "        \n",
    "new_wb.save(my_first_workbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a cell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cell is a distinct collection of attributes and properties at a particular location (identified by a row and column) inside a worksheet. If that definition is too generic, try this:\n",
    "\n",
    "\"The cell is the primary place in which data is stored and operated on. A cell can have a number of characteristics,\n",
    "such as numeric, text, date, or time formatting; alignment; font; color; and a border. Each cell is identified by a\n",
    "cell reference, a combination of its column and row headings.\" ([ECMA OOXML Part 1](http://www.ecma-international.org/publications/standards/Ecma-376.htm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Boss says \"You did great getting that Apple stock data, but I need one worksheet per year.\"\n",
    "#What do?\n",
    "#We could go in and manually separate each year into a different worksheet (from 2014 to 1980). Yuck!\n",
    "#We could do it automatically. Yay!\n",
    "\n",
    "\n",
    "#Basic scheme for the new workbook:\n",
    "# for each year encountered, make a new worksheet\n",
    "# populate that worksheet with the data for that year.\n",
    "aapl_wb = load_workbook(aapl_xlsx)\n",
    "aapl_ws = aapl_wb.active\n",
    "\n",
    "headers = list(aapl_ws['A1': 'G1'])[0]\n",
    "first_data_cell = 'a2'\n",
    "last_data_cell = 'g%s' % (aapl_ws.max_row)\n",
    "#last_data_cell = 'g1000'\n",
    "year = aapl_ws.cell(row=2, column=1).value[:4]\n",
    "\n",
    "aapl_separated_file = \"data/AAPL_separated.xlsx\"\n",
    "aapl_separated_wb = Workbook()\n",
    "\n",
    "ws = aapl_separated_wb.active\n",
    "ws.title = year\n",
    "ws.append([cell.value for cell in headers])\n",
    "\n",
    "new_worksheets = {year: ws}\n",
    "\n",
    "for row in aapl_ws[first_data_cell:last_data_cell]:\n",
    "    #Each of these things is an individual cell\n",
    "    date, p_open, p_high, p_low, p_close, p_vol, p_adj_close = row\n",
    "    year = date.value[:4]\n",
    "    if year not in new_worksheets:\n",
    "        ws = aapl_separated_wb.create_sheet(title=year)\n",
    "        new_worksheets[year] = ws\n",
    "        ws.append([cell.value for cell in headers])\n",
    "        \n",
    "    else:\n",
    "        ws = new_worksheets[year]\n",
    "        \n",
    "    ws.append([cell.value for cell in row])\n",
    "    \n",
    "aapl_separated_wb.save(aapl_separated_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine and Human Readable Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale of difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. JSON (easiest)\n",
    "2. YAML\n",
    "3. XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* JSON is used to great success in programmatic web design (REST APIs for example)\n",
    "* XML is used for heavyweight \n",
    "* YAML for config files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Serialization\n",
    "  * Serialization is the process of translating data structures or object state into a format that can be stored for later reconstruction and use. [Wikipedia](https://en.wikipedia.org/wiki/Serialization)\n",
    "2. Markup Language\n",
    "  * A markup language is a system for annotating a document in a way that is syntactically distinguishable from the text. [Wikipedia](https://en.wikipedia.org/wiki/Markup_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON : JavaScript Object Notation   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is [JSON](http://json.org/)? \n",
    "JSON is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why JSON?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It has a 5 page specification\n",
    "  * Easy to parse, and therefore very fast to parse\n",
    "* It is cross-language (i.e. every major and lots of minor ones has a json encoder and decoder)\n",
    "* Simple structure, and easy to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why not JSON?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No NaN\n",
    "* Everything is a string (which means that information will be lost when converting to JSON)\n",
    "  * You need to keep data types sometimes\n",
    "* The kind of information that can be \"JSONified\" is more limited\n",
    "* Simple structure, can be difficult to represent complex or interdependent structure\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAML: YAML Ain't Markup Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is [YAML](http://www.yaml.org/spec/1.2/spec.html)?\n",
    "YAML is a **data serialization** language, **not** a markup language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why YAML?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* YAML is a superset of JSON\n",
    "  * YAML imposes additional constraints on input data that JSON doesn't, like the uniqueness of keys.\n",
    "* YAML is easy for a human to read\n",
    "* Indentation matters (just like in Python)\n",
    "* It has datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why not YAML?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Not nearly as widely adopted as JSON or XML\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML: eXtensible Markup Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is [XML](http://www.w3.org/TR/2008/REC-xml-20081126/#sec-intro)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why (should you use) XML?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Very stable and capable\n",
    "* Wide adoption\n",
    "* Structure can be pre-defined and enforce with DTDs (Document Type Definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why (should you) not (use) XML?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:\n",
    "* OOXML (Microsoft Office)\n",
    "* ODT\n",
    "* RSS\n",
    "* XHTML\n",
    "* SVG (Scalable Vector Graphics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have lxml installed in your conda environment run\n",
    "```\n",
    "% conda install -y lxml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import lxml.etree\n",
    "\n",
    "json_first = '''{\"libraries\":[\"numpy\", \"scipy\"], \n",
    "               \"dependencies\": [\"fftw\", \"mkl\"], \n",
    "               \"name\":\"my_new_module\"}'''\n",
    "                \n",
    "#What is the difference between json_first and json_second?\n",
    "json_second = {\"libraries\":[\"numpy\", \"scipy\"], \n",
    "               \"dependencies\": [\"fftw\", \"mkl\"], \n",
    "               \"name\":\"my_new_module\"}\n",
    "\n",
    "my_dict = {\"libraries\":[\"numpy\", \"scipy\"], \n",
    "           \"dependencies\": [\"fftw\", \"mkl\"], \n",
    "           \"name\":\"my_new_module\"}\n",
    "\n",
    "yamlized = yaml.dump(my_dict)\n",
    "jsonized = json.dumps(my_dict)\n",
    "\n",
    "xml = lxml.etree.Element(\"module\")\n",
    "xml.append(lxml.etree.Element(\"name\"))\n",
    "xml[-1].text=\"my_new_module\"\n",
    "\n",
    "xml.append(lxml.etree.Element(\"libraries\"))\n",
    "xml[-1].text = \"numpy\"\n",
    "xml.append(lxml.etree.Element(\"libraries\"))\n",
    "xml[-1].text = \"scipy\"\n",
    "\n",
    "xml.append(lxml.etree.Element(\"dependencies\"))\n",
    "xml[-1].text = \"fftw\"\n",
    "xml.append(lxml.etree.Element(\"dependencies\"))\n",
    "xml[-1].text = \"mkl\"\n",
    "\n",
    "xmlized = lxml.etree.tostring(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details at https://docs.python.org/3/library/json.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "notebook = json.load(open('data/notebook.ipynb'))\n",
    "notebook.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notebook['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{cell['cell_type'] for cell in notebook['cells']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "code = [cell['source'] for cell in notebook['cells'] \n",
    "                       if cell['cell_type'] == 'code']\n",
    "for n, block in enumerate(code):\n",
    "    if n < 6:\n",
    "        continue\n",
    "    print(''.join(block))\n",
    "    print('='*65)\n",
    "    if n > 8: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nyc_harbor_file = \"data/nyc_harbor_wq_2006-2014.xlsx\"\n",
    "harbor_data = pd.read_excel(nyc_harbor_file)\n",
    "\n",
    "harbor_row = harbor_data[:1].get_values()\n",
    "myrow = harbor_row.tolist()[0][4:]\n",
    "myrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(pd.isnull(x) for x in myrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dumps(myrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dumps(['foo', {'bar': ('baz', None, 1.0, 2)}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.load(open('data/pyyaml-index.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cat data/pyyaml-index.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details at http://pyyaml.org/wiki/PyYAMLDocumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "%cat data/graphviz-meta.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yaml.load(open('data/graphviz-meta.yaml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn to write YAML by dumping Python objects\n",
    "\n",
    "For example, a list of dictionaries with some None's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(yaml.dump([{'key1': 'a', 'key2': 2, 'list_key': [1, 2, 'abc']}, {'key1': 'b', 'key2': 3, 'list_key': [None,None]}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(yaml.dump(['foo', {'bar': ('baz', None, 1.0, 2)}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some XML files from the HDF5 descriptions info\n",
    "metadata = \"data/Granule_Metadata.xml\"\n",
    "collection = \"data/GES_DISC_GPM_3GPROFF16SSMIS_DAY_V03_dif.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details at https://docs.python.org/3/library/pyexpat.html#module-xml.parsers.expat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.parsers.expat as expat\n",
    "indent = 0  # global variable quick-and-dirty\n",
    "\n",
    "# 3 handler functions\n",
    "def start_element(name, attrs):\n",
    "    global indent\n",
    "    print(\"  \"*indent + 'Start element:', name, attrs)\n",
    "    indent += 1\n",
    "def end_element(name):\n",
    "    global indent\n",
    "    indent -= 1\n",
    "    print(\"  \"*indent + 'End element:', name)\n",
    "def char_data(data):\n",
    "    global indent\n",
    "    print(\"  \"*(indent-1) + 'Character data:', repr(data))\n",
    "\n",
    "p = expat.ParserCreate()\n",
    "p.StartElementHandler = start_element\n",
    "p.EndElementHandler = end_element\n",
    "p.CharacterDataHandler = char_data\n",
    "p.ParseFile(open(metadata, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElementTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details at https://docs.python.org/3/library/xml.etree.elementtree.html#module-xml.etree.ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def print_element(elem, indent=0):\n",
    "    print(\"  \"*indent + \"Start element:\", elem.tag, elem.attrib)\n",
    "    print(\"  \"*indent + \"Character data:\", repr(elem.text))\n",
    "    for child in elem:\n",
    "        print_element(child, indent+1)\n",
    "    if elem.tail:\n",
    "        print(\"  \"*indent + \"Character data:\", repr(elem.tail))\n",
    "    print(\"  \"*indent + \"End element:\", elem.tag)\n",
    "\n",
    "tree = ET.parse(metadata)\n",
    "root = tree.getroot()\n",
    "print_element(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for shortname in root.iter(\"ShortName\"):\n",
    "    print(shortname.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAX (Simple API for XML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details at https://docs.python.org/3/library/xml.sax.html#module-xml.sax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.sax as sax\n",
    "\n",
    "# Similar pull-based style as expat, slightly higher level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOM (Document Object Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details at https://docs.python.org/3/library/xml.dom.html#module-xml.dom\n",
    "\n",
    "Really only use this if you need compatibility in programming style with older code, or with code in other programming languages like Java.  For the Pythonic high-level approach, use `ElementTree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.dom.minidom as DOM\n",
    "dom = DOM.parse(metadata)\n",
    "print(dom.childNodes)\n",
    "root = dom.childNodes[2]\n",
    "print(root.tagName, root.attributes.items())\n",
    "# ... etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (representing and processing XML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the YAML file we looked at from a conda package?  See on your local system:\n",
    "\n",
    "```\n",
    "data/graphviz-meta.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of your conda packages installed on your system have a similar file (i.e. called `meta.yaml` in a package directory).  For this exercise, imagine that Continuum Analytics were transported back in time to the early 2000s, and wanted to change the storage of all this package metadata into an XML format.\n",
    "\n",
    "* Develop the XML dialect to be used to represent the data in this (and similar) YAML files.\n",
    "  * You may define this dialect purely informally.  If you have way too much time, feel free to write a DTD (Document Type Definition), W3C XML Schema, or ISO RELAX NG, formal definitions of the dialect.\n",
    "* Write the content of the mentioned data file as XML in the dialect you developed.\n",
    "* Read the XML you have written out using one of the Python XML parsing libraries discussed.\n",
    "* Write a utility function `get_requirements(meta, type_='build')` that will pull out a list of requirements for a package (either `build`, `conflicts`, or `run`) from your parsed representation of the XML.\n",
    "  * If you have time, write a couple other utility functions that seem useful for working with your format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details at https://www.hdfgroup.org/HDF5/doc/H5.intro.html\n",
    "\n",
    "1. HDF5 files that are accessed via h5py store and return numpy arrays\n",
    "2. HDF5 files are composed groups and datasets\n",
    "3. Storing numerical-ish data is strongly recommended\n",
    "4. Groups can be accessed like both Python dicts and like Unix filesystem paths\n",
    "```python\n",
    "# Full path\n",
    "hdf5_file['/group1/subgroup2/subsubgroup1']\n",
    "# Equivalent to:\n",
    "g = hdf5_file['group1']\n",
    "g['subgroup2/subsubgroup1']\n",
    "# Or to nested lookup:\n",
    "hdf5_file['group1']['subgroup2']['subsubgroup1']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We won't be covering HDF (aka HDF4).\n",
    "  * HDF5 and HDF4 are two different things, even though they are by the same group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 files are composed of **groups** and **datasets**.\n",
    "A group contains any number of groups and datasets plus supporting metadata.\n",
    "A dataset is a multidimensional array of data elements plus supporting metadata.\n",
    "\n",
    "HDF5 files are organized like UNIX paths.\n",
    "Every HDF5 file has a group (the root) at \"/\".\n",
    "\n",
    "HDF5 groups are somewhat similar to Python dicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have problems if you try to use both pytables and h5py at the same time.\n",
    "This has been fixed in recent versions, but some people still use old stuff!!\n",
    "\n",
    "* http://stackoverflow.com/questions/28333470/use-both-h5py-and-pytables-in-the-same-python-process\n",
    "* https://github.com/h5py/h5py/issues/390\n",
    "  \n",
    "**ALWAYS** close the HDF5 file not matter what, after each small sequence of access.  \n",
    "\n",
    "Since merely opening a file doesn't require any reads or writes, it is safest to enclose each operation you wish to perform in a `with open(\"myfile.hdf5\"): ...` block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have h5py installed in your conda environment run\n",
    "```\n",
    "% conda install -y h5py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Let's make a file!\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "filename = \"tmp/my_first_hdf5.hdf5\"\n",
    "\n",
    "# h5py.File can take a driver=\"driver\", libver=\"latest|earliest\", \n",
    "# and userblock=<size> arguments. In general, leave those options alone unless\n",
    "#  - you are using parallel HDF5 (aka MPI). Then set driver=\"mpio\"\n",
    "#  - you have to squeeze every bit of performance from the application, \n",
    "#    and don't care if no-one else can use it. Then set libver=\"latest\"\n",
    "#  - userblock is NOT chunking. userblock is some space at the beginning of the \n",
    "#    file that really isn't a part of the file.\n",
    "my_first_hdf5 = h5py.File(filename, mode='w')\n",
    "my_first_hdf5.close()\n",
    "\n",
    "# Hurray! We made our first (rather boring) hdf5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Put something in the file\n",
    "with h5py.File(filename, mode='w') as my_first_hdf5:\n",
    "    data = list(range(1000))\n",
    "    my_first_hdf5['dataset1'] = data\n",
    "    \n",
    "# This example easily put Python a list into an HDF5 dataset\n",
    "# We can (sort of) put arbitrary Python things into HDF5, but we shouldn't. \n",
    "# What should we store? Numerical-ish things.\n",
    "# What should we not store? Whatever we want.\n",
    "#\n",
    "# Whatever! I do what I want! \n",
    "#   - Eric Cartman (S6E3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Read the data\n",
    "with h5py.File(filename, mode='r') as my_first_hdf5:\n",
    "    data2 = my_first_hdf5['dataset1']\n",
    "\n",
    "print(data2)\n",
    "# Hmmm. Instead of getting the data, we instead got a \"closed HDF5 dataset\".\n",
    "# This is because h5py is lazily loading data instead of loading everything at once.\n",
    "#\n",
    "# This is really good!\n",
    "# What would happen if our dataset was 200GB? Could we load all of that into memory at once?\n",
    "# Probably not. (Unless you are very lucky to have access to a server with that much RAM)\n",
    "# But even if we have the memory, it probably doesn't make sense to load the whole thing \n",
    "# and then start processing it is probably smarter to iteratively load and process the \n",
    "# data in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3a: Actually read the data\n",
    "with h5py.File(filename, mode='r') as my_first_hdf5:\n",
    "    data2 = my_first_hdf5['dataset1'][:]\n",
    "\n",
    "print(type(data2))\n",
    "print(data2[:10])\n",
    "# We put a Python list into the dataset, but got a numpy array out.\n",
    "# Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Let's play with groups\n",
    "with h5py.File(filename, mode='w') as my_first_hdf5:\n",
    "    g1 = my_first_hdf5.create_group(\"first\")\n",
    "    # We can create nested groups automatically\n",
    "    # second, third, and fourth will each be different groups\n",
    "    g2 = my_first_hdf5.create_group(\"second/third/fourth\")\n",
    "    # We can create groups under a previously created group\n",
    "    # Note: g1.create_group instead of my_first_hdf5.create_group\n",
    "    g3 = g1.create_group(\"nestedfirst\")\n",
    "    g4 = g1.create_group(\"nestedsecond\")\n",
    "    #Now the group \"first\" has \n",
    "    \n",
    "    g5 = my_first_hdf5.create_group(\"first/nestedthird\")\n",
    "    \n",
    "# Questions:\n",
    "# Where is group \"first\"? group \"second\"?\n",
    "# How many groups are nested under \"first\"?\n",
    "# What is the absolute path to group \"nestedsecond\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What is a group?\n",
    "# What is a dataset?\n",
    "# Can a group contain another group?\n",
    "# Can a group contain a dataset?\n",
    "    \n",
    "with h5py.File(filename, mode='r') as my_first_hdf5:\n",
    "    list_of_groups = []\n",
    "    # visit() recursively visits every group and dataset in a file\n",
    "    # It calls the function that is given as an argument, stopping\n",
    "    #  if that function returns anything other than None\n",
    "    my_first_hdf5.visit(list_of_groups.append)\n",
    "    #my_first_hdf5.visit(print)\n",
    "\n",
    "list_of_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4a: Let's play with groups\n",
    "with h5py.File(filename, mode='w') as my_first_hdf5:\n",
    "    g1 = my_first_hdf5.create_group(\"first\")\n",
    "    # We can create nested groups automatically\n",
    "    # second, third, and fourth will each be different groups\n",
    "    g2 = my_first_hdf5.create_group(\"second/third/fourth\")\n",
    "    # We can create groups under a previously created group\n",
    "    # Note: g1.create_group instead of my_first_hdf5.create_group\n",
    "    g3 = g1.create_group(\"nestedfirst\")\n",
    "    g4 = g1.create_group(\"nestedsecond\")\n",
    "    # Now the group \"first\" has \n",
    "    \n",
    "    g5 = my_first_hdf5.create_group(\"first/nestedthird\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Where is group \"first\"? group \"second\"?\n",
    "2. How many groups are nested under \"first\"?\n",
    "3. What is the absolute path to group \"nestedsecond\"?\n",
    "4. What is a group?\n",
    "5. What is a dataset?\n",
    "6. Can a group contain another group?\n",
    "7. Can a group contain a dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 5: Combining groups and datasets\n",
    "filename = \"tmp/my_second_hdf5.hdf5\"\n",
    "data = [[i+j*10 for i in range(10)] for j in range(100)]\n",
    "data2 = np.arange(1000).reshape((10,20,5))\n",
    "\n",
    "with h5py.File(filename, mode='w') as f:\n",
    "    g = f.create_group(\"data\")\n",
    "    dset1 = g.create_dataset(\"dataset1\", (100,10), np.dtype('i8'), data=data)\n",
    "    # We could also have done it like so:\n",
    "    # f['data/dataset1'] = data\n",
    "    # What is the difference? create_dataset() is more flexible. It allows us to\n",
    "    #  - specify size and shape\n",
    "    #  - specify datatype\n",
    "    #  - specify chunking\n",
    "    #  - specify transparent compression\n",
    "    #  - specify resizability\n",
    "    dset2 = g.create_dataset(\"dataset2\", data2.shape)\n",
    "    dset2 = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(filename, mode='r') as f:\n",
    "    dset1 = f['data/dataset1'][:]\n",
    "    dset2 = f['data/dataset2'][:]\n",
    "    \n",
    "print(dset1.shape, \"\\n\", dset1[:1])\n",
    "print(dset2.shape, \"\\n\", dset2[:1])\n",
    "# Why is dset2 full of zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(filename, mode='w') as f:\n",
    "    g = f.create_group(\"data\")\n",
    "    # Option 1:\n",
    "    dset2 = g.create_dataset(\"dataset2\", shape=data2.shape, dtype=data2.dtype)\n",
    "    dset2[:] = data2\n",
    "    # The [:] is important!\n",
    "    \n",
    "    # Option 2:\n",
    "    # f['dataset2'] = data2\n",
    "\n",
    "with h5py.File(filename, mode='r') as f:\n",
    "    dset2 = f['data/dataset2'][:]\n",
    "    \n",
    "print(dset1.shape, \"\\n\", dset1[:1])\n",
    "print(dset2.shape, \"\\n\", dset2[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iterating over datasets is also easy.\n",
    "# Remember, each dataset is basically a numpy array that is read from disk on demand\n",
    "with h5py.File(filename, mode='r') as f:\n",
    "    for item in f['data/dataset2']:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 6: Deleting datasets from a file\n",
    "filename = \"tmp/my_third_hdf5.hdf5\"\n",
    "\n",
    "with h5py.File(filename, \"w\") as f:\n",
    "    f['data/dataset1'] = np.arange(100000).reshape(10,10000)\n",
    "\n",
    "%ls -l $filename\n",
    "with h5py.File(filename, \"r+\") as f:\n",
    "    del f['data/dataset1']\n",
    "    %ls -l $filename\n",
    "\n",
    "#The dataset isn't actually deleted until the file is closed\n",
    "%ls -l $filename\n",
    "\n",
    "with h5py.File(filename, \"r+\") as f:\n",
    "    try:\n",
    "        del f['data/dataset1']\n",
    "    except KeyError:\n",
    "        print(\"Trying to delete dataset that doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 6a: Deleting entire groups\n",
    "with h5py.File(filename, \"w\") as f:\n",
    "    f['data/dataset1'] = np.arange(100000).reshape(10,10000)\n",
    "    f['data/dataset2'] = np.arange(100000,200000).reshape(10,10000)\n",
    "    f['data/dataset3'] = np.arange(200000,300000).reshape(10,10000)\n",
    "    \n",
    "%ls -l $filename\n",
    "with h5py.File(filename, \"r+\") as f:\n",
    "    del f['data']\n",
    "    %ls -l $filename\n",
    "\n",
    "# The dataset isn't actually deleted until the file is closed\n",
    "%ls -l $filename\n",
    "\n",
    "with h5py.File(filename, \"r+\") as f:\n",
    "    l = []\n",
    "    f.visit(l.append)\n",
    "\n",
    "# Notice that the file didn't shrink to a small number of bytes.\n",
    "# The datasets and group have been unlinked, but the space hasn't been reclaimed.\n",
    "# To shrink the file, we need to run an \"h5repack\" on it.\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 7: Updating an existing dataset\n",
    "filename = \"tmp/my_fourth_hdf5.hdf5\"\n",
    "\n",
    "with h5py.File(filename, \"w\") as f:\n",
    "    f['data/dataset1'] = np.arange(100000).reshape(10000,10)\n",
    "    f['data/dataset2'] = np.arange(100000,200000).reshape(10000,10)\n",
    "    f['data/dataset3'] = np.arange(200000,300000).reshape(10000,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 7: Updating datasets\n",
    "with h5py.File(filename, \"r+\") as f:\n",
    "    print(f['data/dataset1'][:10])\n",
    "    f['data/dataset1'][:5] = -1\n",
    "    \n",
    "with h5py.File(filename, \"r+\") as f:\n",
    "    print(f['data/dataset1'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 8: resizing existing datasets\n",
    "d1 = np.arange(100000).reshape(10000,10)\n",
    "with h5py.File(filename, \"w\") as f:\n",
    "    # make a new dataset that can grow to 10x the initial size\n",
    "    dset1 = f.create_dataset(\"resizable/dataset1\", d1.shape, \n",
    "                             maxshape=(d1.shape[0]*10, d1.shape[1]))\n",
    "    dset1[:] = d1\n",
    "    \n",
    "    # Here is an alternate way to create the dataset\n",
    "    # f.create_dataset(\"resizable/dataset1\", d1.shape, \n",
    "    #                  maxshape=(d1.shape[0]*10, d1.shape[1]), data=d1)\n",
    "%ls -l $filename    \n",
    "\n",
    "with h5py.File(filename, \"r+\") as f:\n",
    "    # double the size of the dataset\n",
    "    dset1 = f[\"resizable/dataset1\"]\n",
    "    print(dset1.shape)\n",
    "    print(dset1.maxshape)\n",
    "    dset1.resize(dset1.shape[0]*2, axis=0)\n",
    "    print(dset1.shape)\n",
    "    \n",
    "    dset1[dset1.shape[0]//2:] = d1\n",
    "\n",
    "%ls -l $filename\n",
    "with h5py.File(filename, \"r+\") as f:\n",
    "    # Check that the dataset is actually the size we want\n",
    "    dset1 = f[\"resizable/dataset1\"]\n",
    "    d1 = dset1[:]\n",
    "    print(d1.shape)\n",
    "    print(d1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(filename, \"r+\") as f:\n",
    "    # resize again, past our original limit\n",
    "    dset1 = f[\"resizable/dataset1\"]\n",
    "    print(dset1.shape)\n",
    "    print(dset1.maxshape)\n",
    "    dset1.resize(dset1.shape[0]*6, axis=0)\n",
    "    print(dset1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for datasets to be resized, they *must* be chunked.\n",
    "\n",
    "This chunking happens automatically in some cases, but can be specified. Chunking happens automatically when:\n",
    "\n",
    "- compression is turned on\n",
    "- maxshape is specified for the dataset\n",
    "\n",
    "Intuition about chunking\n",
    "\n",
    "- Specifying the chunk size is easy to get wrong! Especially when multiple subtle factors are in play:\n",
    "  - Chunk size\n",
    "  - Compression\n",
    "  - Chunk cache size\n",
    "  - Underlying disk subsystem (especially for parallel filesystems)\n",
    "\n",
    "http://www.hdfgroup.org/HDF5/doc/Advanced/Chunking/\n",
    "http://www.hdfgroup.org/HDF5/doc/Advanced/Chunking/Chunking_Tutorial_EOS13_2009.pdf\n",
    "\n",
    "**If the chunk size is wrong, accessing the data can be 10-100 times slower than normal.**\n",
    "\n",
    "Moral of the story: Don't set chunking yourself unless you can conclusively demonstrate that it is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step 9: HDF5 Attributes on Groups and Datasets\n",
    "#Step 10: Transparent compression\n",
    "# - Why transparent compression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring an HDF5 file found \"in the wild\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "metadata = \"data/Granule_Metadata.xml\"\n",
    "collection = \"data/GES_DISC_GPM_3GPROFF16SSMIS_DAY_V03_dif.xml\"\n",
    "hdf5_precip = \"data/3A-DAY.F16.SSMIS.GRID2014R2.20150101-S000000-E235959.001.V03C.HDF5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import webbrowser, os\n",
    "try:\n",
    "    from urllib.parse import quote\n",
    "except ImportError:\n",
    "    from urllib import quote # Python 2.7\n",
    "webbrowser.open(\"file:///%s/%s\" % (os.getcwd(), quote(metadata)))\n",
    "webbrowser.open(\"file:///%s/%s\" % (os.getcwd(), quote(collection)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = h5py.File(hdf5_precip, \"r\")\n",
    "list(f.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f['InputFileNames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f['InputFileNames'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputFileNames = list(f['InputFileNames'])[0].decode().split(',')\n",
    "inputFileNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_datasets = list(f['Grid'])\n",
    "grid_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rain = f['Grid']['liquidPrecipFraction']\n",
    "print(rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rain[:5,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice a lot of these apparently sentinal values in the datasets. The value -9999.90039062 seems to be used as a filled-in number in a presumably sparse array (the file size isn't large enough to hold all the data if it was non-sparse, as we will see)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us see which datasets have meaningful values, and how commonly\n",
    "from operator import mul\n",
    "from functools import reduce\n",
    "\n",
    "for dataset in grid_datasets:\n",
    "    data = f['Grid'][dataset]\n",
    "    non_sentinal = data[:] >= -9999\n",
    "    print(dataset, \"has real data in %d of %d positions\" % (\n",
    "                    non_sentinal.sum(), reduce(mul, data.shape, 1)))\n",
    "    print(\"-\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(rain[:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(rain[705:716,400:411])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drizzle = (.1 < rain[:]) & (rain[:] < .9)              \n",
    "drizzle.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times = f['InputGenerationDateTimes']\n",
    "times[0].decode('utf-8').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(f.attrs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.attrs['FileInfo'].decode('utf-8').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.attrs['FileHeader'].decode('utf-8').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We've already seen that mixedWater is only those sentinal values\n",
    "# But just want to show how to use a Pandas Panel for N dimensions\n",
    "mixedWater = f['Grid']['mixedWater']\n",
    "panel = pd.Panel(f['Grid']['mixedWater'][:])\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "panel[10:15,700,400:411]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic creation of a new HDF5 data file is done with:\n",
    "\n",
    "```python\n",
    ">>> import h5py\n",
    ">>> import numpy as np\n",
    ">>> f = h5py.File(\"mytestfile.hdf5\", \"w\")\n",
    ">>> dset = f.create_dataset(\"mydataset\", (100,), dtype='i')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetCDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details at http://unidata.github.io/netcdf4-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have netCDF4 installed in your conda environment run\n",
    "```\n",
    "% conda install -y netcdf4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import netCDF4\n",
    "f = netCDF4.Dataset('data/sresa1b_ncar_ccsm3-example.nc')\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.variables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f['pr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f['pr'][:].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f['pr'].dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precip_flux = pd.DataFrame(f['pr'][:].squeeze())\n",
    "precip_flux.columns = f['lon']\n",
    "precip_flux.index = f['lat']\n",
    "precip_flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (export to scientific formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the NYC Harbor data setand perhaps also the normalization work done in the previous exercisesave the data to compact scientific data formats, HDF5 and/or NetCDF. \n",
    "\n",
    "* Take advantage of the option of saving multiple datasets into HDF5 or NetCDF to break down the data.\n",
    "* Store the data in its native types per column/cell (Pandas does a good job of inferring data types)\n",
    "* How large is the resulting HDF5/NetCDF file compared to the original Excel file.\n",
    "* Compose some interesting queries of the database to extract patterns or features of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of HDF5 and NetCDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial at http://docs.h5py.org/en/latest/quick.html is likely to be useful.\n",
    "\n",
    "The basic creation of a new NetCDF data file is done with:\n",
    "\n",
    "```python\n",
    ">>> from netCDF4 import Dataset\n",
    ">>> rootgrp = Dataset(\"test.nc\", \"w\", format=\"NETCDF4\")\n",
    ">>> print rootgrp.data_model\n",
    "NETCDF4\n",
    ">>> rootgrp.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial at http://nbviewer.ipython.org/github/Unidata/netcdf4-python/blob/master/examples/writing_netCDF.ipynb is likely to be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDL .sav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import readsav\n",
    "datafile = \"data/1985_2010_Cedar_Creek_Resident_Fish_Data_for_Analysis.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idl2df(fname, key='o', verbose=True):\n",
    "    \"Read a data frame from IDL; default key is one used by QEA\"\n",
    "    data = readsav(datafile, verbose=verbose)\n",
    "    top = data[key]\n",
    "    columns = top.dtype.names\n",
    "    df = pd.DataFrame(list(zip(*top[0])), columns=columns)\n",
    "    return df\n",
    "\n",
    "def df_bytes2str(df, columns=None, encoding='utf-8'):\n",
    "    columns = columns or df.columns\n",
    "    for col in columns:\n",
    "        if type(df[col][0]) == bytes:\n",
    "            df[col] = df[col].str.decode(encoding)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = idl2df(datafile, verbose=False)\n",
    "df_bytes2str(df)\n",
    "df[df.SPECIES == 'Carp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Work with SQLite3 single-file databases\n",
    "* Work with RDBMS's using the DBAPI standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load some data we'll use for later examples\n",
    "import src.rdb as rdb\n",
    "cowlitz_file = 'data/cowlitz_river_wa_usgs_flow_data.rdb'\n",
    "comment, header, cowlitz_data = rdb.read_rdb(cowlitz_file)\n",
    "\n",
    "# Notice the form of this data is a list of namedtuples\n",
    "print(\"%d rows of data\" % len(cowlitz_data), end='\\n----------\\n')\n",
    "for row in cowlitz_data[:3]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load some data we'll use for later examples\n",
    "# Note the form of this data is a Pandas DataFrame\n",
    "import pandas as pd\n",
    "aapl = pd.read_csv('data/AAPL.csv', index_col='Date')\n",
    "print(\"%d rows of data\" % len(aapl), end='\\n----------\\n')\n",
    "aapl[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    os.remove('tmp/test-db')\n",
    "except OSError:\n",
    "    print(\"File already not there\")\n",
    "#!rm tmp/test-db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "db = sqlite3.connect(\"tmp/test-db\")\n",
    "db.execute(\"create table stocks \"\n",
    "           \"(symbol text, shares integer, price real, \"\n",
    "           \" primary key (symbol))\")\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.execute(\"insert into stocks values (?, ?, ?)\", ('IBM', 50, 91.10))\n",
    "db.execute(\"insert into stocks values (?, ?, ?)\", ('AAPL', 100, 123.45))\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in db.execute(\"select * from stocks\"):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks = [('GOOG', 75, 380.13), ('AA', 100, 14.20), ('AIG', 124, 0.99)]\n",
    "db.executemany(\"insert into stocks values (?, ?, ?)\", stocks)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(db.execute(\"select * from stocks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(db.execute(\"select symbol, price from stocks where shares >= 100\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.execute(\"insert into stocks values (?, ?, ?)\", ('IBM', 100, 124.5))\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%ls -l tmp/test-db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.execute(\"CREATE TABLE cowlitz \"\n",
    "           \"(agency_cd TEXT, site_no INTEGER, date DATE, \"\n",
    "           \" discharge REAL, status TEXT, PRIMARY KEY (date))\")\n",
    "for row in cowlitz_data:\n",
    "    db.execute(\"INSERT INTO cowlitz VALUES (?, ?, ?, ?, ?)\", row)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "est = db.execute('SELECT COUNT(*) FROM cowlitz WHERE status=\"A:e\"')\n",
    "list(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for d in db.execute('SELECT * FROM cowlitz WHERE '\n",
    "                    'date >= \"1988-01-01\" AND date < \"1988-01-10\"'):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%ls -l tmp/test-db\n",
    "%ls -l $cowlitz_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need Pandas column names to be valid SQL column names\n",
    "aapl['Adj_Close'] = aapl['Adj Close']\n",
    "del aapl['Adj Close']\n",
    "aapl[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aapl.to_sql('AAPL', db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in db.execute(\"SELECT * FROM AAPL LIMIT 10\"):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostgreSQL (and DBAPI generally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2 # maybe import oracledb, mysql, db2\n",
    "conn = psycopg2.connect(database='test', user='dmertz')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('SELECT version()')\n",
    "version = cursor.fetchone()\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"drop table stocks\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"create table stocks \"\n",
    "               \"(symbol text, shares integer, price real, \"\n",
    "               \"primary key (symbol))\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"insert into stocks values (%s, %s, %s)\", \n",
    "               ('IBM', 50, 91.10))\n",
    "cursor.execute(\"insert into stocks values (%s, %s, %s)\", \n",
    "               ('AAPL', 100, 123.45))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"select * from stocks;\")\n",
    "for row in cursor:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks = [('GOOG', 75, 380.13), ('AA', 100, 14.20), ('AIG', 124, 0.99)]\n",
    "cursor.executemany(\"insert into stocks values (%s, %s, %s)\", stocks)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"select * from stocks\")\n",
    "list(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"select column_name, data_type, character_maximum_length \"\n",
    "               \"from INFORMATION_SCHEMA.COLUMNS \"\n",
    "               \"where table_name = 'stocks'\")\n",
    "list(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"select symbol, price from stocks where shares >= 100\")\n",
    "lots_of_shares = cursor.fetchall()\n",
    "lots_of_shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    cursor.execute(\"insert into stocks values (%s, %s, %s)\", \n",
    "                   ('IBM', 100, 124.5))\n",
    "finally:\n",
    "    conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    cursor.execute(\"drop table cowlitz\")\n",
    "except psycopg2.ProgrammingError:\n",
    "    print(\"Table does not exist... create below\")\n",
    "finally:\n",
    "    conn.commit()\n",
    "cursor.execute(\"CREATE TABLE cowlitz \"\n",
    "               \"(agency_cd TEXT, site_no INTEGER, date DATE, \"\n",
    "               \" discharge REAL, status TEXT, PRIMARY KEY (date))\")\n",
    "cursor.executemany(\"INSERT INTO cowlitz VALUES (%s, %s, %s, %s, %s)\", \n",
    "                   cowlitz_data)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT COUNT(*) FROM cowlitz WHERE status = 'A:e'\")\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT * FROM cowlitz WHERE \"\n",
    "               \"date >= '1988-01-01' AND date < '1988-01-10'\")\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fortran 77 Unformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import FortranFile\n",
    "fortran_raw = \"data/gratsr-fortran.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ff = FortranFile(fortran_raw, 'r')\n",
    "print(ff.read_reals(dtype=np.float16))\n",
    "print(ff.read_record(dtype=[('X', np.float32)]))\n",
    "#ff.read_record(dtype=[('BT', '<f4')])\n",
    "print(ff.read_record(dtype=[('X', '<a1')]))\n",
    "print(ff.read_record(dtype=[('X', '<f4')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/copyright.png'>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda_training]",
   "language": "python",
   "name": "conda-env-anaconda_training-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
